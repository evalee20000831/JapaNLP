{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5d126c",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Model \n",
    "This project aims to generate informal Japanese to formal Japanese (Keigo). Although I am holding N1 Japanese-Language Proficiency Test, it is still difficult for me to write in formal Japanese (Keigo). This thought inspires me to start this project. Before I started the project, I saw this [wordrabbit company](https://wordrabbit.jp/) that does exactly what this project is aiming to do and plus correcting grammars. The company name is provided as a reference. \n",
    "\n",
    "There are mutiple sites and code examples that support me to build this model: \n",
    "* [bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) -> most of the reference is from here for seq2seq model\n",
    "* [NLP From Scratch: Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#loading-data-files)\n",
    "* [日本語テキストの前処理：neologdn、大文字小文字、Unicode正規化](https://tuttieee.hatenablog.com/entry/ja-nlp-preprocess)\n",
    "* [Hironsan/natural-language-preprocessings](https://github.com/Hironsan/natural-language-preprocessings/blob/master/preprocessings/ja/cleaning.py)\n",
    "* [社会的関係を考慮した日本語敬語コーパスの構築への取り組み](https://www.jstage.jst.go.jp/article/pjsai/JSAI2022/0/JSAI2022_2C1GS601/_article/-char/ja/)\n",
    "* [敬語変換タスクにおける評価用データセット](https://github.com/cl-tohoku/keigo_transfer_task?tab=readme-ov-file)\n",
    "\n",
    "Also, if you want to make a reference to this code, please feel free to do so. HOWEVER, **rememeber to read the conclusion section before you refer to this project**.\n",
    "\n",
    "*Disclaimer: this is for personal project (self-improvement) only, not for commercial use.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HILbaeo-b8pA",
   "metadata": {
    "id": "HILbaeo-b8pA"
   },
   "source": [
    "## Prerequisite\n",
    "The following cell contains all the libraries you need to run this project. If any of this doesn't work, you can refer to the next two cell blocks which help you with the software setup. Two types of setup are available - iOS and Google Colab. Both installations are through pip. If any of the import failed above, use the corresponding command line to install. \n",
    "\n",
    "*   [MeCab](https://pypi.org/project/mecab-python3/)\n",
    "*   [neologdn](https://pypi.org/project/neologdn/)\n",
    "*   [evaluate](https://pypi.org/project/evaluate/)\n",
    "\n",
    "*Note that this all use pip to install. If you want to use other method, you have to look it up.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d83d8cd-02ee-4e6f-99ce-3400cd5df005",
   "metadata": {
    "id": "2d83d8cd-02ee-4e6f-99ce-3400cd5df005"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eva.lee/Library/Python/3.11/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import MeCab # morphological analyzer\n",
    "import re\n",
    "import csv\n",
    "import neologdn # Japanese text normalizer\n",
    "import requests # handles url tasks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import torchtext\n",
    "import evaluate\n",
    "# from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cca6a9d-56cf-4588-b388-3b1f8b83b8b5",
   "metadata": {
    "id": "4cca6a9d-56cf-4588-b388-3b1f8b83b8b5"
   },
   "outputs": [],
   "source": [
    "# iOS\n",
    "# pip3 install -U scikit-learn\n",
    "# pip3 install torch torchvision torchaudio\n",
    "# pip3 install pandas\n",
    "# pip3 install torchtext\n",
    "# pip3 install evaluate\n",
    "# pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "UtbpMd1Xbyfe",
   "metadata": {
    "id": "UtbpMd1Xbyfe"
   },
   "outputs": [],
   "source": [
    "# Google Colab\n",
    "# %pip install mecab-python3\n",
    "# %pip install unidic-lite\n",
    "# %pip install langdetect\n",
    "# %pip install -U deep-translator\n",
    "# %pip install neologdn\n",
    "# %pip install --user -U nltk\n",
    "# %pip install spacy_download\n",
    "# %pip install evaluate\n",
    "# %pip install nltk\n",
    "# !python3 -m spacy download ja_core_news_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad4be1-6417-43c1-b3df-f6b90f217b63",
   "metadata": {},
   "source": [
    "## Retrive Data \n",
    "Data is retrive from a csv file. This created database is mostly coming from [日本語学習者支援のための敬語変換タスクの提案](https://github.com/cl-tohoku/keigo_transfer_task?tab=readme-ov-file). I have also contributed part of the dataset to increase the volumn of the database. \n",
    "\n",
    "Dataset [日本語学習者支援のための敬語変換タスクの提案](https://github.com/cl-tohoku/keigo_transfer_task?tab=readme-ov-file) was formed by Touhouky University to study keigo translation. The first column is original sentence - which could be either informal or formal (keigo); The second column is formal (keigo) sentences; The third column is the notation of the translation that indicates the level of transformation from original sentence to formal (keigo) sentence. The third column is not used in this project, because knowing the level of transformation is not the goal of the project. \n",
    "\n",
    "For the sentences I contributed to the database, I grabbed the sentences from newspapers and articles, asked ChatGPT to translate, then reviewed and edited the sentences myself.  \n",
    "\n",
    "In total, there is **~850 data** in the csv file. This is not a lot of data, but this is the only open source dataset I could find at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5698198b-08dd-449f-ba0b-9195f542619d",
   "metadata": {
    "id": "5698198b-08dd-449f-ba0b-9195f542619d"
   },
   "outputs": [],
   "source": [
    "def reader():\n",
    "    original_sen = []\n",
    "    transform_sen = []\n",
    "\n",
    "    with open('dataset.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        counter = 0\n",
    "\n",
    "        for row in csv_reader:\n",
    "            if len(row)==2 or (counter != 0): \n",
    "                original_sen.append(row[0])\n",
    "                transform_sen.append(row[1])\n",
    "            counter+=1\n",
    "\n",
    "    return (original_sen, transform_sen)\n",
    "\n",
    "def list_to_file(x, y, type):\n",
    "    df = pd.DataFrame({'X_' + type: x, 'y_' + type: y})\n",
    "    filename = type + '_data.csv'\n",
    "    df.to_csv(filename) # overwrites everytime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bd128-b3c5-476c-a813-6fb8ff5eb459",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "The data preprocessing has 3 parts: \n",
    "* **Data Cleaning**: removes the unneeded, corrupted notation in the given sentence. \n",
    "* **Normalization**: removes any numbers in the given sentence. \n",
    "* **Tokenization** : makes given sentence into segments of text in a meaningful way. I tried 2 kinds of tokenization library - spacy and [MeCab](https://taku910.github.io/mecab/). [MeCab](https://taku910.github.io/mecab/) gives a better meaningful segments, so I chose to use [MeCab](https://taku910.github.io/mecab/) instead of popular spacy library. In this Tokenization process, I also included `<sos>` and `<eos>` when tokens are made for each sentence. \n",
    "\n",
    "Other data preprocessing method doesn't make sense to apply for this project / dataset. For example, *Stop Words Removal* method is not used, because Japanese language doesn't really have any stop words. *Lemmatization* method does not apply either, because we are evaluating the transformation of informal Japanese to formal (keigo) Japanese. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aac68b1-4c61-48ed-9e75-be788beb2d31",
   "metadata": {
    "id": "8aac68b1-4c61-48ed-9e75-be788beb2d31"
   },
   "outputs": [],
   "source": [
    "# Data Cleaning \n",
    "def data_cleaning(text):\n",
    "    text = text.replace(\"\\\"\", \"\") # remove double quote\n",
    "    text = re.sub(\"[\\uFF01-\\uFF0F\\uFF1A-\\uFF20\\uFF3B-\\uFF40\\uFF5B-\\uFF65\\u3000-\\u303F\\u2460-\\u2473]\", \"\", text) # remove special characters\n",
    "    text = neologdn.normalize(text) # this includes a lot of special characters removal\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Normalization \n",
    "def normalization(text):\n",
    "    text = re.sub(r\"\\d+\", \"0\", text) # delete numbers (set to 0)\n",
    "    return text\n",
    "\n",
    "def data_preprocessing(data_list): \n",
    "    preprocessed_data = [] \n",
    "    \n",
    "    for data in data_list:\n",
    "        cleaned_data = data_cleaning(data)\n",
    "        normalized_data = normalization(cleaned_data)        \n",
    "        preprocessed_data.append(normalized_data)\n",
    "    \n",
    "        # tokens = [sos_token] + tokenization(normalized_data) + [eos_token]# tokenization\n",
    "        # print(\"Token:\" ,tokens)\n",
    "\n",
    "        # vacab = torchtext.vocab.build_vocab_from_iterator(tokens)\n",
    "        # print(vacab.get_itos())\n",
    "        # tokenization_spacy(normalized_data)\n",
    "\n",
    "    return preprocessed_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80580f7-f5b7-4f52-bdd1-0343a6614532",
   "metadata": {
    "id": "a80580f7-f5b7-4f52-bdd1-0343a6614532"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenization(text):\n",
    "    wakati = MeCab.Tagger(\"-Owakati\")\n",
    "    tokenized_data = wakati.parse(text).split()\n",
    "    return tokenized_data\n",
    "\n",
    "# Tokenization with Spacy\n",
    "def tokenization_spacy(text):\n",
    "    nlp = spacy.load(\"ja_core_news_sm\") # Spacy's trained pipeline for Japanese - Language Object\n",
    "    tokenized_data = []\n",
    "    for token in nlp.tokenizer(text):\n",
    "        tokenized_data.append(token)\n",
    "    return nlp.tokenizer(text)\n",
    "\n",
    "# Accepts a list of strings (sentences) to do tokenization \n",
    "def token_generator(text_list, sos_token, eos_token):\n",
    "    for text in text_list:\n",
    "        yield [sos_token] + tokenization(text) + [eos_token]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8f1f4-5448-4bd8-bddd-ecb0ad35c3e1",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "This is example with both x (informal) and y (formal) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366a51e6-3737-4dab-9bdd-2d3fd125f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_4 = \"週末街に出た\"\n",
    "sentence_4_trg = \"週末街に出てきました\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ff8a040-b223-4d86-8cc8-ff7bae47fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['週末街に出た'] ; After: ['週末街に出た']\n",
      "Before: ['週末街に出てきました'] ; After: ['週末街に出てきました']\n"
     ]
    }
   ],
   "source": [
    "X_train_sample = [sentence_4]\n",
    "y_train_sample = [sentence_4_trg]\n",
    "\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "preprocessed_x_sample = data_preprocessing(X_train_sample) \n",
    "preprocessed_y_sample = data_preprocessing(y_train_sample) \n",
    "print(\"Before:\", X_train_sample, \"; After:\", preprocessed_x_sample)\n",
    "print(\"Before:\", y_train_sample, \"; After:\", preprocessed_y_sample)\n",
    "\n",
    "\n",
    "tokenized_x_sample = token_generator(preprocessed_x_sample, sos_token, eos_token)# tokenization\n",
    "tokenized_y_sample = token_generator(preprocessed_y_sample, sos_token, eos_token)# tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02137c23-db87-4adb-9b29-a4dc1d097c21",
   "metadata": {},
   "source": [
    "### Simple Example 2\n",
    "I pulled out some sentences from the dataset to show how the data preprocessing is done for better understanding.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b61e982-e43f-4a7f-ad95-a9521f263452",
   "metadata": {
    "id": "9b61e982-e43f-4a7f-ad95-a9521f263452"
   },
   "outputs": [],
   "source": [
    "example = \"朝ごはんはトーストにバターとべジマイトを薄くぬって食べました。\"\n",
    "example_2 = \"お医者さんに会ったけど彼は「あなたは元気ですが、ティーンエイジャーだからいつも眠いのですね。」と言いました。\"\n",
    "example_3 = \"①中国の漁船が日本の海上保安庁の船に故意に激突（これについては証拠のビデオがあるそうです）\"\n",
    "example_4 = \"ちなみに、台湾の人気ウェブは\\\"台灣論壇\\\"や\\\"台大PTT(BBS)\\\"です。\"\n",
    "example_5 = \"Ｐａｌｍ　Ｓｐｒｉｎｇｓでは　沢山のＲｅｓｏｒｔが　あります。\"\n",
    "example_6 = \"明日、友達と一緒に町へ行ってきます。もっとネイルチップがほしいです（笑）。\"\n",
    "example_7 = \"神様だけが知っている、秘密～～～～♫　あはははっははははははっははははｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋ\"\n",
    "example_8 = \"東京部の最低賃金は１時間で７９１円だが、アメリカは１時間で７．２５ドルだ 。\"\n",
    "example_9 = \"だから私は Manga Moods と呼ばれる別の本を借りました。\"\n",
    "example_9 = \"だから私は Manga Moods と呼ばれる別の本を借りました。\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "321e8976-34de-4e11-a65c-0228b36bdbe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "321e8976-34de-4e11-a65c-0228b36bdbe7",
    "outputId": "93a4360f-3b99-40ce-f221-fd01e7923008",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 朝ごはんはトーストにバターとべジマイトを薄くぬって食べました。\n",
      "After: 朝ごはんはトーストにバターとべジマイトを薄くぬって食べました \n",
      "\n",
      "Before: お医者さんに会ったけど彼は「あなたは元気ですが、ティーンエイジャーだからいつも眠いのですね。」と言いました。\n",
      "After: お医者さんに会ったけど彼はあなたは元気ですがティーンエイジャーだからいつも眠いのですねと言いました \n",
      "\n",
      "Before: ①中国の漁船が日本の海上保安庁の船に故意に激突（これについては証拠のビデオがあるそうです）\n",
      "After: 中国の漁船が日本の海上保安庁の船に故意に激突これについては証拠のビデオがあるそうです \n",
      "\n",
      "Before: ちなみに、台湾の人気ウェブは\"台灣論壇\"や\"台大PTT(BBS)\"です。\n",
      "After: ちなみに台湾の人気ウェブは台灣論壇や台大ptt(bbs)です \n",
      "\n",
      "Before: Ｐａｌｍ　Ｓｐｒｉｎｇｓでは　沢山のＲｅｓｏｒｔが　あります。\n",
      "After: palmspringsでは沢山のresortがあります \n",
      "\n",
      "Before: 明日、友達と一緒に町へ行ってきます。もっとネイルチップがほしいです（笑）。\n",
      "After: 明日友達と一緒に町へ行ってきますもっとネイルチップがほしいです笑 \n",
      "\n",
      "Before: 神様だけが知っている、秘密～～～～♫　あはははっははははははっははははｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋｋ\n",
      "After: 神様だけが知っている秘密♫あはははっははははははっははははkkkkkkkkkkkkkkkkkkkkkkkkkkkk \n",
      "\n",
      "Before: 東京部の最低賃金は１時間で７９１円だが、アメリカは１時間で７．２５ドルだ 。\n",
      "After: 東京部の最低賃金は0時間で0円だがアメリカは0時間で0ドルだ \n",
      "\n",
      "Before: だから私は Manga Moods と呼ばれる別の本を借りました。\n",
      "After: だから私はmanga moodsと呼ばれる別の本を借りました \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing with some samples\n",
    "X_train = [example, example_2, example_3, example_4, example_5,\n",
    "        example_6, example_7, example_8, example_9]\n",
    "\n",
    "preprocessed_data = [] \n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "for text in X_train:\n",
    "    tokenization_spacy(text)\n",
    " \n",
    "preprocessed_data = data_preprocessing(X_train) # # accepts a list of strings (sentences) to do data preprocessing\n",
    "\n",
    "for i in range(len(X_train)): \n",
    "    print(\"Before:\", X_train[i])\n",
    "    print(\"After:\", preprocessed_data[i], \"\\n\")\n",
    "\n",
    "tokenized_data = token_generator(preprocessed_data, sos_token, eos_token) # accepts a list of strings (sentences) to do tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4848ab14-c5d9-4e41-977a-1c4035ac169b",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing\n",
    "This section uses the same method to do data preprocessing for dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91f0e2bc-61ba-4862-860f-79ff35cb9585",
   "metadata": {
    "id": "91f0e2bc-61ba-4862-860f-79ff35cb9585",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pre-processing is done\n"
     ]
    }
   ],
   "source": [
    "original_sen, transform_sen = reader() # get dataset \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(original_sen, transform_sen, test_size = 0.2, random_state = 0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 0)\n",
    "\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "preprocessed_x = data_preprocessing(X_train) \n",
    "preprocessed_y = data_preprocessing(y_train)\n",
    "\n",
    "preprocessed_x_val = data_preprocessing(X_val) \n",
    "preprocessed_y_val = data_preprocessing(y_val)\n",
    "\n",
    "preprocessed_x_test = data_preprocessing(X_test) \n",
    "preprocessed_y_test = data_preprocessing(y_test)\n",
    "\n",
    "tokenized_x = token_generator(preprocessed_x, sos_token, eos_token)# tokenization\n",
    "tokenized_y = token_generator(preprocessed_y, sos_token, eos_token)# tokenization\n",
    "\n",
    "\n",
    "# store train and test data into csv file for better visualization\n",
    "list_to_file(preprocessed_x, preprocessed_y, \"train\")\n",
    "list_to_file(preprocessed_x_val, preprocessed_y_val, \"test\")\n",
    "list_to_file(preprocessed_x_test, preprocessed_y_test, \"validate\")\n",
    "\n",
    "print(\"Data pre-processing is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e383fca-ecc3-4074-83b5-4d8a6cb266ad",
   "metadata": {},
   "source": [
    "## Dataset Creation \n",
    "To create dataset, I am following this [Dataset & Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) data primitives to define and process my data for later model usage. Dataset stores the samples (formal / informal sentences) and their corresponding labels (formal / informal). DataLoader wraps an iterable around the Dataset to enable easy access to the samples. \n",
    "\n",
    "In this section, only setting up dataset is done. DataLoader is done at the following section. \n",
    "\n",
    "To build the vocabularies from all the available tokens in the dataset, I am utilizing this `build_vocab_from_iterator` from `torchtext.vocab`. You can read the details for `torchtext.vocab` [here](https://pytorch.org/text/stable/vocab.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58fc0b49-82ea-4f60-beeb-3028eb664497",
   "metadata": {
    "id": "58fc0b49-82ea-4f60-beeb-3028eb664497"
   },
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Retrieves dataset’s source and target sentence one sample at a time.\n",
    "    \"\"\"\n",
    "    def __init__(self, source_data, target_data, src_vocab, trg_vocab):\n",
    "        self.source_data = source_data # list of informal language\n",
    "        self.target_data = target_data # list of formal langauge\n",
    "\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        # total number of data input\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Loads and returns a sample from the dataset at the given index idx\n",
    "        \"\"\"\n",
    "        source_text = self.source_data[index]\n",
    "        target_text = self.target_data[index]\n",
    "\n",
    "        src_word_to_index = self.numericalized(source_text, self.src_vocab)\n",
    "        tar_word_to_index = self.numericalized(target_text, self.trg_vocab)\n",
    "        \n",
    "        # use torch.tensor() instead of torch.Tensor() to retain data type \n",
    "        return torch.tensor(src_word_to_index), torch.tensor(tar_word_to_index)\n",
    "\n",
    "    def numericalized(self, sentence, vocab):\n",
    "        \"\"\"\n",
    "        Converts word to index based on formal/informal vocab\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = tokenization(sentence) # list\n",
    "        word_to_index = []\n",
    "\n",
    "        for t in tokens:\n",
    "            word_to_index.append(vocab[t])\n",
    "\n",
    "        return word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea095ddb-98db-46fd-b625-18c43a0e6618",
   "metadata": {
    "id": "ea095ddb-98db-46fd-b625-18c43a0e6618"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Function \n",
    "\n",
    "src_display: transform int-type or tensor-type to words for train x \n",
    "\n",
    "trg_display: transform int-type or tensor-type to words for train y \n",
    "\"\"\" \n",
    "def src_display(x_list_indices):\n",
    "    print(\"\\nsrc (indice -> word):\", end = \" \")\n",
    "\n",
    "    if torch.is_tensor(x_list_indices[0]):\n",
    "        for index in x_list_indices:\n",
    "            print(train_X_vocab.get_itos()[index.int()], end = \" \")\n",
    "    else:\n",
    "        for index in x_list_indices:\n",
    "            print(train_X_vocab.get_itos()[index], end = \" \")\n",
    "\n",
    "def trg_display(y_list_indices):\n",
    "    print(\"\\ntrg (indice -> word):\", end = \" \")\n",
    "\n",
    "    if torch.is_tensor(y_list_indices[0]):\n",
    "        for index in y_list_indices:\n",
    "            print(train_y_vocab.get_itos()[index.int()], end = \" \")\n",
    "    else:\n",
    "        for index in y_list_indices:\n",
    "            print(train_y_vocab.get_itos()[index], end = \" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed6d5bd-b45e-481b-ac74-71e54fec96ac",
   "metadata": {},
   "source": [
    "### Simple Example \n",
    "Continue with the simple example from the top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4bbe37-9dbc-4bf8-aeb6-d20e698fc039",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "min_freq=0\n",
    "\n",
    "special_token = [unk_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "train_X_vocab_sample = build_vocab_from_iterator(tokenized_x_sample, specials=special_token, min_freq=min_freq) # vocab of informal language\n",
    "train_y_vocab_sample = build_vocab_from_iterator(tokenized_y_sample, specials=special_token, min_freq=min_freq) # vocab of formal language\n",
    "\n",
    "train_X_vocab_sample.set_default_index(train_X_vocab_sample[unk_token])\n",
    "train_y_vocab_sample.set_default_index(train_y_vocab_sample[unk_token])\n",
    "\n",
    "train_dataset_sample = Dataset(X_train_sample, y_train_sample, train_X_vocab_sample, train_y_vocab_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9898e40-fff0-45ad-a216-f01ce90781c4",
   "metadata": {},
   "source": [
    "#### Display for X data on Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80ed9746-a3ee-4a36-9b6b-787608442e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '週末', '街', 'に', '出', 'た', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "for i in token_generator(X_train_sample, sos_token, eos_token):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53f150c6-60af-4f25-8f3d-b629526a6326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 9\n",
      "Word-to-index mapping: {'週末': 8, '出': 6, '<sos>': 2, '<eos>': 3, '<unk>': 0, '<pad>': 1, '街': 7, 'に': 5, 'た': 4}\n",
      "Index-to-word mapping: ['<unk>', '<pad>', '<sos>', '<eos>', 'た', 'に', '出', '街', '週末']\n",
      "\n",
      "Index at 4 is た\n",
      "Existing word 'に' has index 5\n",
      "Non-Existing word 'sh' returns [0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size\", len(train_X_vocab_sample))\n",
    "print(\"Word-to-index mapping:\", train_X_vocab_sample.get_stoi())\n",
    "print(\"Index-to-word mapping:\", train_X_vocab_sample.get_itos())\n",
    "\n",
    "print(\"\\nIndex at 4 is\", train_X_vocab_sample.get_itos()[4])\n",
    "print(\"Existing word 'に' has index\", train_X_vocab_sample.get_stoi()[\"に\"])\n",
    "print(\"Non-Existing word 'sh' returns\", train_X_vocab_sample.lookup_indices([\"sh\"])) # unknown word, returns 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e772850-f3a3-4c65-98a5-5978c81afdb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index-to-word mapping (X): ['<unk>', '<pad>', '<sos>', '<eos>', 'た', 'に', '出', '街', '週末']\n",
      "Index-to-word mapping (Y): ['<unk>', '<pad>', '<sos>', '<eos>', 'き', 'た', 'て', 'に', 'まし', '出', '街', '週末']\n",
      "\n",
      "Word-to-index mapping (X): {'週末': 8, '出': 6, '<sos>': 2, '<eos>': 3, '<unk>': 0, '<pad>': 1, '街': 7, 'に': 5, 'た': 4}\n",
      "Word-to-index mapping (Y): {'週末': 11, '出': 9, 'まし': 8, '街': 10, 'に': 7, 'た': 5, 'き': 4, '<sos>': 2, '<pad>': 1, 'て': 6, '<eos>': 3, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIndex-to-word mapping (X):\", train_X_vocab_sample.get_itos())\n",
    "print(\"Index-to-word mapping (Y):\", train_y_vocab_sample.get_itos())\n",
    "\n",
    "print(\"\\nWord-to-index mapping (X):\", train_X_vocab_sample.get_stoi())\n",
    "print(\"Word-to-index mapping (Y):\", train_y_vocab_sample.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb1f84db-5f8d-4d84-a7d6-4e32320388c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train x (src) sentence: 週末街に出た\n",
      "Train y (trg) sentence: 週末街に出てきました\n",
      "\n",
      "Variable train_dataset is type  <class '__main__.Dataset'>\n",
      "Each dataset in train_dataset is <class 'torch.Tensor'>\n",
      "type of a tensorflow: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Examples of data struction and type \n",
    "print(\"Train x (src) sentence:\", X_train_sample[0]) \n",
    "print(\"Train y (trg) sentence:\", y_train_sample[0])\n",
    "print(\"\\nVariable train_dataset is type \", type(train_dataset_sample))\n",
    "\n",
    "src_tensor_sample, trg_tensor_sample = train_dataset_sample[0] # tensor type \n",
    "print(\"Each dataset in train_dataset is\", type(src_tensor_sample))\n",
    "print(\"type of a tensorflow:\", src_tensor_sample.dtype) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3b5ff7-84cf-4d97-98f5-9113cb9833e2",
   "metadata": {},
   "source": [
    "### Dataset Creation \n",
    "This section uses the same method to create dataset. \n",
    "\n",
    "The ratio of train:test:validate is 0.8:0.16:0.04. Only vocabularies / tokens from train dataset is needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca335ca8-8a15-465c-9b96-d8fe8f74d59f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca335ca8-8a15-465c-9b96-d8fe8f74d59f",
    "outputId": "517c56c9-e08b-4236-b79f-7c144669b490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_token = [unk_token, pad_token, sos_token, eos_token]\n",
    "min_freq=0\n",
    "\n",
    "# Train\n",
    "train_X_vocab = build_vocab_from_iterator(tokenized_x, specials=special_token, min_freq=min_freq) # vocab of informal language\n",
    "train_y_vocab = build_vocab_from_iterator(tokenized_y, specials=special_token, min_freq=min_freq) # vocab of formal language\n",
    "\n",
    "train_X_vocab.set_default_index(train_X_vocab[unk_token])\n",
    "train_y_vocab.set_default_index(train_y_vocab[unk_token])\n",
    "train_dataset = Dataset(preprocessed_x, preprocessed_y, train_X_vocab, train_y_vocab)\n",
    "\n",
    "# Validate\n",
    "valid_dataset = Dataset(preprocessed_x_val, preprocessed_y_val, train_X_vocab, train_y_vocab)\n",
    "\n",
    "# Test\n",
    "test_dataset = Dataset(preprocessed_x_test, preprocessed_y_test, train_X_vocab, train_y_vocab)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d4554-899f-4ea1-b9de-fc3acc3cdf2d",
   "metadata": {},
   "source": [
    "#### Display for X and y data on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afc77a3f-35e0-42ea-ad66-393c7fea4040",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afc77a3f-35e0-42ea-ad66-393c7fea4040",
    "outputId": "9ddf86ba-bca2-45d4-fa4a-df9e6cd34ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train x (src) sentence at index 1: 私も親切で砕けた人ですから、何時までも何か悩みがあるとしたら、私にご相談していただければ、とても嬉しいと思いますね。\n",
      "Train y (trg) sentence at index 1: 私も親切で砕けた人ですから、何時までも何か悩みがあるとしたら、私にご相談いただければ、とても嬉しいと思いますね。\n",
      "\n",
      "Variable train_dataset is type  <class '__main__.Dataset'>\n",
      "Each dataset in train_dataset is <class 'torch.Tensor'>\n",
      "type of a tensorflow: torch.int64\n",
      "tensor([  25,   14, 1205,   12, 1952,    8,   26,   15,   23,   79,   49,   94,\n",
      "          14,   79,   20, 1783,    9,   42,   11,   13,   38,   25,    5,  698,\n",
      "         609,   13,    7, 1348,   66,   74, 1724,   11,   54,   17,   88])\n",
      "\n",
      "src (indice -> word): 私 も 親切 で 砕け た 人 です から 何 時 まで も 何 か 悩み が ある と し たら 私 に ご 相談 し て いただけれ ば とても 嬉しい と 思い ます ね \n",
      "trg (indice -> word): 私 も 親切 で 砕け た 人 です から 何 時 まで も 何 か 悩み が ある と し たら 私 に ご 相談 いただけれ ば とても 嬉しい と 思い ます ね "
     ]
    }
   ],
   "source": [
    "# Examples of data struction and type \n",
    "print(\"Train x (src) sentence at index 1:\", X_train[1]) \n",
    "print(\"Train y (trg) sentence at index 1:\", y_train[1])\n",
    "print(\"\\nVariable train_dataset is type \", type(train_dataset))\n",
    "\n",
    "src_tensor, trg_tensor = train_dataset[1] # tensor type \n",
    "print(\"Each dataset in train_dataset is\", type(src_tensor))\n",
    "print(\"type of a tensorflow:\", src_tensor.dtype) \n",
    "\n",
    "print(src_tensor)\n",
    "src_display(src_tensor)\n",
    "trg_display(trg_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b88b1f8-0635-40fc-bac3-efea804c8828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for src_tensor, trg_tensor in train_dataset:\n",
    "#     src_display(src_tensor)\n",
    "#     trg_display(trg_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50820bb6-f04e-4127-80ce-74738a74480e",
   "metadata": {
    "id": "50820bb6-f04e-4127-80ce-74738a74480e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of X_train: 2112\n",
      "Vocabulary size of y_train: 2136\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size of X_train:\", len(train_X_vocab))\n",
    "print(\"Vocabulary size of y_train:\", len(train_y_vocab))\n",
    "\n",
    "# print(\"\\nWord-to-index mapping:\", train_X_vocab.get_stoi()) # string to integer\n",
    "# print(\"\\nIndex-to-word mapping:\", train_X_vocab.get_itos()) # integer to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ccbf0b6-dd4e-44c6-9403-ce592497f50f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ccbf0b6-dd4e-44c6-9403-ce592497f50f",
    "outputId": "d42a4568-d6d7-4166-b2b0-72834c1eaead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index at 4 is の\n",
      "Existing word 'に' has index 5\n",
      "Non-Existing word 'sh' returns [0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIndex at 4 is\", train_X_vocab.get_itos()[4]) # indice to word example \n",
    "print(\"Existing word 'に' has index\", train_X_vocab.get_stoi()[\"に\"]) # existing word to indice \n",
    "print(\"Non-Existing word 'sh' returns\", train_X_vocab.lookup_indices([\"sh\"])) \n",
    "                                                # unknown word, returns 0 when lookup_indices function is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fe01a15-e32c-4fd4-9a94-7882e14eedc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index-to-word mapping (X): ['<unk>', '<pad>', '<sos>', '<eos>', 'の', 'に', 'は', 'て', 'た', 'が', 'を', 'と', 'で', 'し', 'も', 'です', 'まし', 'ます', 'ない', 'な', 'か', '日本', 'こと', 'から', 'い', '私', '人', 'だ', 'ん', 'する', 'いる', 'いう', '0', '語', 'さん', 'ませ', 'あり', '.', 'たら', 'けど', 'よう', 'れ', 'ある', 'この', 'それ', '中', 'でし', '日', 'その', '時', 'いい', 'さ', 'なく', '中国', '思い', 'なっ', 'お', 'でき', '・', '自分', '文化', 'だけ', 'なかっ', 'たい', 'どう', 'や', 'ば', '一', '好き', 'ちょっと', 'よく', '一番', '友達', 'そして', 'とても', '映画', 'たち', 'もの', '今', '何', '言葉', 'そう', 'ため', 'だっ', '先生', '花', 'き', 'なり', 'ね', 'アメリカ', '皆', 'いつ', 'たくさん', 'たり', 'まで', 'み', 'られ', '今日', '気', 'しれ', 'とっ', 'なる', '一緒', '後', '見', '(', ')', 'できる', 'なら', 'よ', '年', '持っ', '方', '的', '若者', 'って', 'なけれ', 'まだ', '前', '勉強', '彼女', '考え', '行っ', 'つい', '国', '書い', '者', 'ず', 'せ', 'だろう', 'など', '僕', '思っ', '時間', '本', '社会', '聞い', '言い', '言っ', 'こんな', 'でしょう', 'へ', 'もう', 'もし', 'アニメ', 'エントリー', 'クラス', 'ストーリー', '人気', '伝統', '外国', '授業', '本当', 'あっ', 'うち', 'すぎ', 'ところ', 'より', 'コメント', '上', '二', '二人', '会社', '全然', '字幕', '家族', '思う', '政府', '旅行', '理解', '病', '話', '辞書', 'くれ', 'そんな', 'てる', 'とき', 'ほど', 'やら', 'ファッション', 'ヤンキー', '両国', '信じ', '大丈夫', '大切', '学生', '家', '庁', '彼', '書く', '最後', '枝', '流行', '漁船', '牡丹', '結婚', '虎', '見る', '訳', '食べ', '!', 'いろいろ', 'ください', 'けれど', 'しまい', 'すぐ', 'ずっと', 'たろう', 'つ', 'ほか', 'まず', 'まま', 'みたい', 'みんな', 'れる', 'インターネット', 'ゲーム', '住ん', '側', '公園', '大', '大好き', '失敗', '対し', '度', '式', '意味', '昔', '最初', '月', '毎日', '水俣', '菓子', '違い', '\"', 'edison', 'japanese', 'lang', 'いけ', 'こう', 'ここ', 'これ', 'じゃ', 'すごく', 'ちゃん', 'ちゃんと', 'どこ', 'どんな', 'もらっ', 'やっぱり', 'よる', 'わから', 'わけ', 'タバコ', 'テレビ', 'ドラマ', 'ネット', '不思議', '今回', '写真', '出し', '問題', '国内', '大学', '委員', '婆', '必要', '教育', '明日', '森', '機', '歌', '母', '沿岸', '津波', '翻訳', '聞く', '船', '英語', '行く', '言わ', '誕生', '進学', '震災', 'ceo', 'あまり', 'お前', 'くらい', 'しか', 'たぶん', 'とも', 'ながら', 'なん', 'なんか', 'ほう', 'ほとんど', 'また', 'もっと', 'やっ', 'ゆうた', 'ら', 'アトラクション', '不安', '世界', '中日', '交流', '使っ', '入っ', '入る', '公', '出来', '単語', '原作', '名前', '夢', '女性', '子供', '学校', '実', '小米', '帰っ', '弟', '影響', '忘れる', '所', '手', '探し', '数', '春樹', '時期', '最近', '村上', '東京', '楽しい', '次', '湖', '漢字', '物', '病気', '目', '県', '答え', '約', '美しい', '裁判', '親友', '買い', '赤ちゃん', '過ごし', '違う', '金持ち', '難しい', '頭', '高校', '-', 'あ', 'いえ', 'いただき', 'いっ', 'いま', 'おい', 'そこ', 'たとえば', 'ちなみ', 'ちゃっ', 'つれ', 'なし', 'ばかり', 'むしろ', 'やがて', 'らしい', 'わかる', 'オーロラ', 'カメラ', 'カード', 'クイーンズランド', 'クリスマス', 'セリフ', 'バイク', 'バージョン', 'ビール', 'プリン', 'プレゼント', 'モデル', 'ロシア', 'ロリータ', '不', '以上', '例', '保安', '兄', '出', '分から', '匹', '原因', '参加', '反抗', '叔母', '同じ', '吸わ', '哀しき', '回', '困っ', '国際', '変え', '外', '夜', '大きい', '大変', '大島', '天', '妹', '始め', '始める', '姿', '少なく', '州', '庵', '待ち', '心', '悲しい', '感じ', '抗議', '故意', '新婚', '施', '春節', '時代', '普通', '書き', '書け', '有名', '有理', '来', '来る', '楽器', '樂', '沢山', '浮かん', '海上', '特に', '生存', '疲れ', '発令', '目的', '相手', '確か', '空港', '答える', '給料', '練習', '置く', '耐', '自信', '花嫁', '行き', '行わ', '表示', '覚え', '観光', '試験', '話し', '説', '警報', '賃金', '質問', '車', '運動', '閉じ', '間', '関係', '風', '飛行', '館', 'k', '…', 'あたし', 'あと', 'あの', 'あるいは', 'いっぱい', 'うれしい', 'かけ', 'くれる', 'ぐらい', 'さらに', 'しばらく', 'しよう', 'せい', 'せる', 'ぜんぜん', 'ただ', 'だんだん', 'ちゃい', 'ちょうど', 'にくい', 'ばあ', 'まるで', 'もっとも', 'やはり', 'やり', 'わ', 'わかっ', 'わたし', 'カバン', 'クラスメイト', 'クラブ', 'クレジット', 'ゴールドコースト', 'サイゴン', 'サブタイトル', 'スタイル', 'スロバキア', 'ソフトウェア', 'チッソ', 'ニュース', 'バグ', 'ビデオ', 'ブーム', 'メルボルン', 'レポート', '一人', '一部', '七', '不明', '両方', '今年', '今週', '以来', '会長', '使う', '偶然', '八百', '内容', '分かる', '分析', '制度', '助け', '労働', '勇気', '動物', '化粧', '北海道', '十', '半分', '可能', '台湾', '号', '味わっ', '命令', '咲き', '四', '圧力', '場所', '変わっ', '夏休み', '天気', '太陽', '失っ', '奥', '女の子', '嬉しかっ', '字', '実家', '宮城', '宿題', '小説', '屋上', '川', '年上', '年間', '幸福', '忘れ', '思い出さ', '恋', '恥ずかしく', '意識', '愛し', '感動', '我慢', '戻っ', '戻り', '批判', '揺れ', '撮っ', '政治', '文字', '文章', '料理', '新しい', '最低', '株式', '歌詞', '歩い', '歩く', '気持ち', '求め', '決め', '注意', '洞窟', '渡し', '演じ', '漫画', '無', '特徴', '王', '班', '理由', '発表', '相談', '確認', '空間', '突然', '管理', '絶対', '続い', '聞か', '自国', '自身', '船長', '色', '花婿', '行け', '行こう', '見つけ', '読み', '貴女', '身', '遊ぶ', '遊ん', '達', '選挙', '部分', '金', '長', '長官', '開封', '間違い', '韓国', '音', '音楽', '順番', '頑張る', ',', 'abraham', 'avcon', 'away', 'd', 'lincoln', 'mikkom', 'oo', 'palmsprings', 'racecar', 'resort', 'secret', 'spirited', 'stratford', 'subtitles', 'tin', 'top', 'whistle', 'with', '☆', 'あなた', 'いき', 'いや', 'いら', 'いらっしゃっ', 'いろんな', 'うえ', 'うまい', 'えいきょう', 'おいしい', 'おかげ', 'おき', 'おば', 'おり', 'お腹', 'かい', 'かかり', 'かかわら', 'かたかな', 'かつて', 'かわいい', 'かわいそう', 'かわり', 'かんじ', 'がる', 'きっと', 'きゃく', 'きり', 'きれ', 'く', 'くわえ', 'けいた', 'けんさ', 'こ', 'この頃', 'ご', 'ごろ', 'さい', 'さえ', 'さっき', 'しかも', 'しっかり', 'しゃ', 'じ', 'すき', 'すごい', 'すごし', 'すっ', 'すばらしい', 'すみ', 'すらすら', 'すん', 'せいげん', 'せめて', 'せんかい', 'たいてい', 'たび', 'だめ', 'ちゅう', 'ついに', 'つうじ', 'つづけ', 'つまら', 'つもり', 'とうとう', 'とにかく', 'とまり', 'とりあえず', 'どっち', 'どの', 'なかなか', 'なれ', 'にくかっ', 'はじめ', 'はっきり', 'はははは', 'ひと', 'ひとり', 'ひどい', 'ひゃっ', 'ぶつかる', 'へん', 'べ', 'べき', 'べつ', 'べんきょう', 'ほうれん', 'ほし', 'ま', 'ましょう', 'まだまだ', 'まったく', 'まね', 'むかし', 'むだ', 'めげ', 'めでとう', 'もともと', 'もらえ', 'もん', 'やっと', 'やめ', 'やめる', 'やりとり', 'やる', 'やれ', 'わざわざ', 'わるい', 'アイルランド', 'アカウント', 'アクション', 'アジア', 'アパート', 'アブラハム', 'アブリル', 'アン', 'インターンシップ', 'ウィ', 'ウェブ', 'エジソン', 'エッセイ', 'エラー', 'オートバイ', 'カナダ', 'カ月', 'ガイ', 'ガール', 'キャノン', 'キュウリ', 'キロ', 'キング', 'ギター', 'クーランガッタ', 'ゲイツ', 'コスプレ', 'コンクール', 'コンサート', 'コンピューター', 'シュワルツェネッガー', 'ジェット', 'スカイプ', 'スキャン', 'スタローン', 'ストラト', 'センテンス', 'タイ', 'タクシー', 'ダメ', 'チケット', 'ツッコミ', 'ツーホー・ゴク・ハースアン・ラン', 'ティン', 'テーマ', 'デー', 'ドル', 'ナイフ', 'ニンジン', 'バドミントン', 'バレンタイン', 'パーティー', 'ヒョンソンア', 'ビル', 'ビーチ', 'ピアノ', 'ファン', 'フォード', 'フリフリ', 'フレア', 'ページ', 'ホイッスル', 'ボール', 'マジ', 'マネジャー', 'マヨネーズ', 'メッセンジャー', 'メディア', 'モスクワ', 'ユース', 'ラビン', 'ラブ・ストーリーラブ・ストーリー', 'リスタート', 'リフレッシュ', 'リンカーン', 'レイ', 'レストラン', 'ロッカー', 'ヴィジュアル', '一下', '一切', '一味', '一夜', '一応', '一気', '一言', '万物', '三', '上手', '上海', '下', '不定', '不振', '与える', '中古', '主役', '乗', '乗っ', '乗る', '予想', '亮', '他人', '付く', '付け', '代わり', '代講', '以', '会', '会え', '住民', '体育', '作っ', '作り', '作る', '作れ', '使い', '使える', '來', '依然', '俳優', '借用', '優しい', '元気', '兆し', '先', '先兵', '先日', '光', '入ら', '入院', '全部', '共存', '共通', '兵士', '内', '円', '冊', '再び', '冒険', '冗談', '出る', '出身', '分野', '切っ', '刘伯', '初め', '別', '別れ', '利益', '制服', '則天', '前半', '力', '助かり', '励まし', '動詞', '北京', '十分', '半期', '卒業', '単純', '厳冬', '及び', '取ら', '受ける', '受付', '古', '古び', '古代', '台', '号令', '司会', '各', '各県', '同じく', '同志', '同様', '名所', '名門', '向かおう', '向け', '呼ば', '和食', '咲く', '員', '回復', '困る', '図書', '国交', '国会', '地', '城', '報', '場合', '塾', '壊れ', '変', '変更', '多い', '多少', '大会', '大堂', '大声', '大統領', '大賞', '天下', '太っ', '太平', '夫婦', '失う', '失明', '失調', '姉', '存分', '季節', '学', '学科', '学習', '守る', '官僚', '宝石', '客', '寂し', '寂しい', '寛容', '寝', '封印', '尋ねる', '小', '小さい', '小南', '少し', '少女', '届き', '岩手', '島', '差し上げ', '希望', '帰', '年齢', '幸い', '広がっ', '広場', '店', '府', '弟子', '張っ', '弾', '役', '後輩', '復興', '心配', '快', '怒り', '思い出す', '思い浮かべる', '性', '恐れる', '愛する', '愛可', '感', '感心', '感想', '感覚', '成績', '我', '手伝おう', '手元', '手足', '払い', '抜け', '担当', '招待', '拝読', '持ち', '挑戦', '挙げる', '捉え', '捧げ', '探察', '提出', '撮り', '撮影', '改めて', '放棄', '故事', '文', '文句', '断然', '断言', '新', '新た', '新聞', '新鮮', '既に', '日にち', '日間', '旦那', '早く', '昨日', '時点', '晩ご飯', '暖かい', '月曜', '有', '服', '朝', '本性', '村', '杯', '東', '東南', '東方', '格好', '格差', '楽しかっ', '楽しめる', '構わ', '様子', '横転', '機動', '次回', '次第', '歌い上げ', '歌っ', '歌手', '武', '残っ', '民間', '気象', '気質', '永遠', '法', '洋', '洞', '海', '海外', '消化', '涼しい', '深', '混乱', '渡っ', '温', '演奏', '激突', '火', '火星', '点', '為', '無料', '無理', '特色', '狭い', '猛威', '猫', '現れ', '現在', '生き', '生きる', '生日', '男女', '町', '留学', '症', '症候', '症状', '痛み', '痺れ', '発生', '発音', '盛ん', '直す', '真剣', '着る', '知っ', '知ら', '知れ', '知識', '祝い', '祝え', '祝日', '神経', '神話', '禁止', '福島', '秘密', '程度', '空', '立ち', '立つ', '立場', '競争', '第', '筋', '系', '紙', '細か', '紹介', '終わっ', '組ま', '組み立て', '経済', '続く', '綺麗', '群', '習い', '耳', '聞き取っ', '聞き取り', '胃痛', '能力', '脳', '腐敗', '興味', '興奮', '色鮮やか', '芝居', '花粉', '苛め', '若い', '苺', '茨城', '草', '萎び', '落とし', '著しい', '行う', '装備', '西洋', '見え', '見える', '見つめ', '視点', '親', '親切', '親族', '観', '解説', '言う', '訂正', '討論', '訪問', '証拠', '証言', '試し', '詰め込む', '話せ', '話せる', '話題', '誇っ', '読む', '読め', '読める', '読ん', '誰', '請求', '警視', '議員', '豆腐', '財産', '責任', '買う', '買っ', '賞', '赤', '起こし', '足り', '路線', '転ん', '軽い', '近づく', '返事', '迫っ', '追う', '追加', '退学', '逆', '週末', '運転', '過激', '道', '道路', '違っ', '遠から', '適当', '選ぶ', '選べ', '選ん', '那覇', '部', '部首', '都合', '重', '金賞', '釣魚', '鋭く', '長く', '際立つ', '障害', '隠そう', '離婚', '難聴', '雪', '雰囲気', '零', '電話', '霍', '青森', '頼み', '食べる', '食べ物', '飲み物', '飾っ', '餌', '騒がす', '驚き', '鳥渡', '黄', \"'\", '---', ':', '?', '`', 'a', 'alfa', 'amazon', 'an', 'as', 'assistant', 'association', 'bissenpuntomay', 'cappi', 'cd', 'doctor', 'for', 'function', 'funny', 'grammar', 'handbook', 'help', 'i', 'is', 'jsa', 'kkkkkkkkkkkkkkkkkkkkkkkkk', 'koreatown', 'la', 'm', 'manga', 'moods', 'my', 'need', 'nict', 'of', 'paper', 'pm', 'student', 'teacher', 'thomas', 'topic', 'torchwood', 'what', 'who', 'words', 'work', 'wto', 'yankee', '♫', 'あきらめ', 'あさって', 'あちこち', 'あっさり', 'あつい', 'あはははっ', 'あれ', 'いがみ合う', 'いく', 'いずれ', 'いただけれ', 'いっさい', 'いったい', 'いらいら', 'うっすら', 'うまく', 'うれしかっ', 'うん', 'えいが', 'えいご', 'おかしく', 'おじ', 'おずおず', 'おはよう', 'かき', 'かく', 'かた', 'かなり', 'かん', 'がたい', 'がたく', 'きっぱり', 'きよく', 'きらめく', 'きれい', 'げ', 'こそ', 'こっそり', 'ころ', 'さて', 'さらさら', 'ざる', 'しかし', 'しっ', 'しまう', 'しまっ', 'しょっちゅう', 'し上がる', 'すぎる', 'すげー', 'すべて', 'せいこう', 'せせらぎ', 'せりふ', 'ぜ', 'そば', 'たかっ', 'たしか', 'たちばな', 'たとえ', 'だいがく', 'だるい', 'だれ', 'っ', 'つけよう', 'つける', 'つながっ', 'つのら', 'つまり', 'づらく', 'できれ', 'とる', 'とんでも', 'なあ', 'なかれ', 'なじら', 'なぜ', 'なれる', 'なんて', 'ぬっ', 'のみ', 'はじめて', 'はつめい', 'はは', 'はまら', 'はやっ', 'ひどく', 'ひねっ', 'ひま', 'ひよこ豆', 'びっくり', 'びんぼう', 'ぶっ飛ばし', 'ぶり', 'べから', 'ほしい', 'ぼう', 'ぼんやり', 'まい', 'ますます', 'まずい', 'まっすぐ', 'まるっきり', 'みよう', 'みる', 'めっちゃ', 'もたらし', 'もちろん', 'もっ', 'もらい', 'やとわ', 'やば', 'ゆう', 'ゆめゆめ', 'ゆるがせ', 'よい', 'よかっ', 'よっ', 'らしく', 'り', 'わかん', 'わや', 'んで', 'アセトアルデヒド', 'アドバイス', 'アルバイト', 'アース', 'イエロマニキュア', 'イト', 'イベント', 'インビクタス', 'エレクトリック', 'オン', 'カキコミ', 'カラー', 'キレイ', 'ギクシャク', 'クラシック', 'コスチューム', 'コテージ', 'コミュニケーション', 'コンボ', 'コール', 'サポータ', 'サーバー', 'シェフ', 'システム', 'シャル', 'ショー', 'シンボル', 'ジオグラフィ', 'ジマ', 'ジョノサン', 'スパイス', 'スピーチ', 'チップ', 'ティーンエイジャー', 'データ', 'トースト', 'トーマスアルファエジソン', 'ドラム', 'ニンニク', 'ネイル', 'ハノイ', 'バイオロジー', 'バス', 'バター', 'バンド', 'バーベキュー', 'パスタ', 'パソコン', 'パーク', 'パープル', 'ヒェン・ツク', 'ファイル', 'ファスト', 'フア・ビー・バン', 'フィルム', 'フェスティバル', 'フレーズ', 'フード', 'ヘビ', 'ベル', 'ホステス', 'ホスト', 'ポスト', 'マシーン', 'ママ', 'ミュージシャン', 'メッセージ', 'メーク', 'メール', 'モノ', 'モンゴル', 'ユーザ', 'ユーモラス', 'ライブ', 'ライン', 'ラブ', 'リーダー', 'ル', 'レ', 'レア', 'レー', 'ロック', 'ローマ', 'ヶ月', 'ー', '一人ぼっち', '一体', '一所', '一時', '一杯', '一生', '一目散', '一瞬', '一般', '一輝', '万博', '丈', '上がっ', '上訴', '上限', '下がっ', '不可解', '世の中', '世論', '両親', '中央', '中学', '中心', '中部', '中高生', '主', '主人', '主流', '乗り物', '乗務', '乗客', '予測', '予約', '事', '事件', '事故', '二三', '五', '亡くなり', '交通', '京劇', '今夜', '今晩', '仕え', '仕事', '仕業', '他', '代', '以下', '仰が', '仲人', '休み', '休暇', '会い', '会っ', '会話', '伴う', '伴っ', '伸ばし', '伸ばせ', '似', '低', '低い', '余裕', '作ら', '使命', '使用', '例えば', '信じる', '信用', '倍', '借り', '停車', '働い', '儲け', '元', '先進', '光っ', '克服', '入り口', '全体', '公開', '其', '内部', '円高', '再現', '出会い', '出会っ', '出演', '分', '分かっ', '分り', '分子', '切っ掛け', '切符', '列車', '初日', '初期', '判断', '別れる', '利', '刻ま', '劉', '加害', '動い', '北', '北村', '北東', '北部', '医者', '午前', '午後', '半間', '協力', '危機', '卵', '厄介', '厳しく', '去年', '反論', '取っ', '取り持っ', '受け', '受け入れ', '台風', '右', '各地', '合え', '同名', '同時', '同社', '同窓', '名利', '吹き', '吹く', '呼ん', '咲い', '品性', '商売', '商業', '喋れる', '喫茶', '嗅い', '嘔吐', '回転', '国語', '園', '土地', '土曜', '土産', '圭吾', '地域', '地震', '基づい', '報道', '墓', '墓穴', '増強', '壁', '声', '変える', '変わら', '外れ', '外食', '多く', '夜中', '大き', '大きな', '大器', '大方', '天津', '奨学', '女', '好き好き', '妨害', '始まっ', '婚', '嫌い', '嬉しい', '学ぶ', '学期', '守り', '守護', '安全', '完了', '完全', '完成', '宗一郎', '官職', '定め', '宝くじ', '実ら', '宮崎', '害', '小さな', '小川', '小麦', '少ない', '尾', '居直っ', '届い', '届け', '山', '工事', '工作', '左', '左側', '帰り', '帰る', '平均', '年収', '幻想', '広い', '広がる', '床', '座っ', '引き上げ', '引く', '弱い', '強', '弾く', '当たっ', '形態', '彼氏', '従っ', '得', '微か', '微笑ん', '心地よかっ', '忙し', '忠実', '思い出し', '思わ', '急', '恋人', '恥', '恩人', '悩み', '悪い', '悪く', '悲しく', '情け深く', '想像', '愛', '愛さ', '愛し合い', '愛国', '慌て', '慎重', '懸命', '成功', '戦争', '戸惑う', '手伝っ', '手続き', '技術', '拒否', '拝ん', '指摘', '振り出し', '掃除', '排列', '排除', '掘っ', '接触', '揉ん', '揚げ餅', '搬送', '支払い', '支持', '故郷', '教え', '教室', '教授', '敬服', '敬語', '料', '断り', '断る', '方法', '日焼け', '昨夜', '晩成', '普段', '景色', '暑い', '暑く', '暗く', '暗記', '書か', '最も', '最高', '有り', '有利', '朝ごはん', '木', '本気', '本田', '材料', '来す', '東洋', '東野', '松雪', '果たす', '枯れ', '染まる', '柴崎', '根拠', '楽し', '楽しみ', '概念', '様', '欧米', '欲しい', '歌え', '歌舞伎', '止まっ', '止め', '正直', '歩け', '死亡', '残念', '殺到', '毎', '比べ', '比較', '民', '民衆', '気まずい', '気力', '水', '汚染', '汽車', '泣い', '泣き', '泰子', '泳い', '泳ぐ', '津', '活動', '派手', '浮かべ', '浮気', '深い', '添削', '済ま', '温まっ', '満たさ', '準備', '漁夫', '漁民', '漂っ', '漏れ', '火垂る', '災い', '無理やり', '無視', '照りつけ', '煽ら', '爆発', '片側', '片田舎', '牢獄', '物価', '特訓', '状態', '状況', '独立', '獣医', '率い', '現地', '現場', '現象', '環境', '生', '生まれ', '生まれ育っ', '生成', '生活', '生産', '田舎', '申告', '画面', '畢竟', '番', '番組', '異', '疎遠', '疑っ', '疑わ', '疾風', '病院', '発展', '発見', '発達', '監督', '目覚しい', '直線', '相', '真', '真似', '眠い', '着い', '睦まじく', '知る', '砂浜', '砕け', '神', '福山', '秒', '究極', '立っ', '笑', '笑顔', '箇所', '簡単', '粉', '素朴', '細い', '紺', '組め', '経験', '結果', '継続', '緊張', '総', '緯度', '縁', '縁石', '群がっ', '老眼', '老齢', '考えれ', '聞き', '聞き出し', '聞き取れ', '聞こえ', '肉眼', '腰', '自然', '至る', '至る所', '至上', '良かっ', '良く', '若', '苦手', '英', '草花', '菊花', '萌え', '落ち', '葉', '薄く', '行か', '行き届い', '行為', '街', '衣装', '表面', '裏', '裏表', '製品', '見なし', '見れ', '見物', '規模', '観測', '解釈', '言え', '言論', '計', '訳す', '証明', '話す', '詳しい', '誤記', '誼', '調べ', '論', '諦め', '諸国', '負わ', '買い物', '費', '赤く', '赤黒く', '起こす', '超え', '足', '距離', '身受け', '車内', '車線', '軍', '軍事', '軍備', '輝く', '輸出', '農民', '辿っ', '迅雷', '近い', '近く', '近年', '迷う', '迷っ', '送り', '送信', '途中', '通じ', '通り', '通信', '通告', '通行', '通関', '速', '連携', '進ん', '遅い', '遅れ', '遊べる', '運ん', '違反', '遠い', '釈放', '釣り合う', '鉱石', '長い', '門', '閉め', '開ける', '開催', '開発', '間違っ', '関し', '限り', '限度', '雄武', '集まる', '離れ', '難しく', '雲', '電信', '電子', '震度', '青', '非', '非常', '面白い', '音痴', '頑固', '頭痛', '額', '飛び込ん', '食欲', '馬', '馬鹿', '駅', '駒込', '驚い', '高い', '髪型', '鮮明', '黒く', '鼻']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nIndex-to-word mapping (X):\", train_X_vocab.get_itos())\n",
    "# print(\"\\nIndex-to-word mapping (X):\", train_y_vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a70e9-4ac9-4363-9978-9a2808492bb9",
   "metadata": {},
   "source": [
    "## Seq-to-Seq Model \n",
    "With all the dataset ready for model, I am defining seq-to-seq model in this section. All the detailed explanation is inside each cell. \n",
    "* Encoder\n",
    "* Decoder\n",
    "* Seq2Seq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c15e11fa-a7b4-43d6-a4e3-dab5731b3baa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c15e11fa-a7b4-43d6-a4e3-dab5731b3baa",
    "outputId": "21318210-1f8b-47e9-9d9d-b8451a5239a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Definition Done\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        :param int input_size: size of input vocab\n",
    "        :param int embedding_dim: size of each embedding/feature vector\n",
    "        :param int hidden_dim: dimensionality of the hidden and cell states\n",
    "        :param int n_layers: num of layers in RNN\n",
    "\n",
    "        :nn.LTSM - multi-layer rnn to an input sequence\n",
    "                    > returns output, (h_n, c_n)\n",
    "                    > output: sequence length, batch size, Hout\n",
    "                    > h_n: final hidden state\n",
    "                    > c_n: final cell state\n",
    "\n",
    "        :nn.Embedding - stores embedding of a fixed dict and size\n",
    "        :nn.Dropout - randomly zeroes some of the elements of the input tensor with probability p, aiming to prevent overfitting\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.hidden = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Encoder RNN provides the initial hidden state for Decoder RNN\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "            # embedded = [input_size, batch_size, embedding_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "            # output = [output_size, batch_size, hidden_dim] \n",
    "            # hidden = [n_layers, batch_size, hidden_dim]\n",
    "            # cell   = [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # print(\"E embedded - \", embedded.shape)\n",
    "        # print(\"E output - \", output.shape)\n",
    "        # print(\"E hidden - \", hidden.shape)\n",
    "        # print(\"E cell - \", cell.shape)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "print(\"Encoder Definition Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cd6d969-b823-4aca-9596-de0356ece910",
   "metadata": {
    "id": "3cd6d969-b823-4aca-9596-de0356ece910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Definition Done\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        Decoder Lanaguage Model generates target sentence, conditioned on encoding\n",
    "\n",
    "        :param int input_size: size of the target vocab\n",
    "        :param int embedding_dim: size of each embedding/feature vector\n",
    "        :param int hidden_dim: dimensionality of the hidden and cell states\n",
    "        :param int n_layers: num of layers in RNN\n",
    "\n",
    "        :nn.LTSM - multi-layer rnn to an input sequence\n",
    "                    > returns output, (h_n, c_n)\n",
    "                    > output: sequence length, batch size, Hout\n",
    "                    > h_n: final hidden state\n",
    "                    > c_n: final cell state\n",
    "\n",
    "        :nn.Embedding - stores embedding of a fixed dict and size\n",
    "        :nn.Dropout - randomly zeroes some of the elements of the input tensor with probability p, aiming to prevent overfitting\n",
    "        :nn.Linear - linear transformation.\n",
    "                    > takes size of each input size, size of each output size\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.hidden = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.ltsm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_size = output_size\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        \"\"\"\n",
    "        :param input : [batch size] \n",
    "        :param hidden: the initial hidden state is provided by Encoder\n",
    "        :param cell  : the intital cell state is provided by Encoder\n",
    "\n",
    "        :unsqueeze(0): adds a dimension to the tensor\n",
    "        \"\"\"\n",
    "        input = input.unsqueeze(0) \n",
    "            # input: [batch_size] -> [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "            # embedded = [1, batch_size, embedding_dim] \n",
    "        output, (hidden, cell) = self.ltsm(embedded, (hidden, cell))\n",
    "            # output = [1, batch_size, output_size] \n",
    "            # hidden = [2, batch_size, output_size]\n",
    "            # cell   = [2, batch_size, output_size]\n",
    "        predict = self.linear(output.squeeze(0))\n",
    "            # predict = [batch_size, output_size]\n",
    "\n",
    "        # print(\"D embedded - \", embedded.shape)\n",
    "        # print(\"D output - \", output.shape)\n",
    "        # print(\"D hidden - \", hidden.shape)\n",
    "        # print(\"D cell - \", cell.shape)\n",
    "        # print(\"D predict - \", predict.shape)\n",
    "        \n",
    "        return predict, hidden, cell\n",
    "\n",
    "print(\"Decoder Definition Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "951c3963-553b-4c57-b291-432937cdf541",
   "metadata": {
    "id": "951c3963-553b-4c57-b291-432937cdf541"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # print(\"Seq src shape:\", source.shape)\n",
    "        # print()\n",
    "        # print(\"Seq trg shape:\", target.shape)\n",
    "        trg_length = target.shape[0] \n",
    "\n",
    "        outputs = torch.zeros(trg_length, target.shape[1], self.decoder.output_size)\n",
    "                              # target_length  # batch_size     # target_vocab_size\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        input = target[0, :] # batch size\n",
    "\n",
    "        for i in range(1, trg_length): # 1 to target_length # 0 is <sos>\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[i] = output\n",
    "            input = output.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ba4a4-a3d0-4868-9295-c6d31ac18df3",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Now All Encoder, Decoder, and Seq2Seq has been defined, we can start intializing our model. \n",
    "\n",
    "This model has 2 layers as suggested by mutiple sources ([Standford online course](https://youtube.com/playlist?list=PLoROMvodv4rMFqRtEuo6SGjY4XbRIVRd4&si=SYTMKNq3x64AwVDN) and [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf)). \n",
    "\n",
    "My dataset is really small, so I am not using too large of the hidden dimension and embedding dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cfa43-4f3e-47d0-b0fe-684cfada8195",
   "metadata": {},
   "source": [
    "### Model Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea84c7e1-25ce-4ef8-b59e-747e837a8ea7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea84c7e1-25ce-4ef8-b59e-747e837a8ea7",
    "outputId": "81f71d69-355c-417c-d635-642684c11c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model setup done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "hidden dimensionality: the optimal size of the hidden layer is usually between the size of the input and size of the output layers\n",
    "\n",
    "number of layers: optimal is 2~4 for encoder and 4 layers for decoder\n",
    "\n",
    "\"\"\"\n",
    "input_size = len(train_X_vocab)\n",
    "output_size = len(train_y_vocab)\n",
    "\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "encoder = Encoder(input_size, embedding_dim, hidden_dim, n_layers, dropout)\n",
    "decoder = Decoder(output_size, embedding_dim, hidden_dim, n_layers, dropout)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "print(\"model setup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f943ad8d-671f-4f8a-92f1-7be9534c13bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train x vocab= 2112\n",
      "Size of train y vocab= 2136\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of train x vocab=\", input_size)  # size of informal japanese vocab \n",
    "print(\"Size of train y vocab=\", output_size) # size of formal japanese vocab "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7593e210-bab9-4029-a14f-cdccfbe98ca5",
   "metadata": {},
   "source": [
    "### Weight Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a25be280-17e1-4101-8749-b24edddc6a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(2112, 64)\n",
       "    (lstm): LSTM(64, 128, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2136, 64)\n",
       "    (ltsm): LSTM(64, 128, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear): Linear(in_features=128, out_features=2136, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        # print(\"name:\" , name)\n",
    "        # print(\"param:\" , param)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23d4fe-7aca-43fd-852b-ef172f7e8499",
   "metadata": {},
   "source": [
    "### Dataloader, Optimizer & Loss Function \n",
    "In this Dataset creation section above, I mentioned that I am following this [Dataset & Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) structure. In this section, I built `DataLoader` which helps with iterating through the dataset. I have three dataset - train, validate, and test. \n",
    "\n",
    "Function `customed_collate_fn` is also created along with `Dataloader` as sugguested by this [github project](https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) and [Custom datasets in Pytorch — Part 2. Text (Machine Translation)\n",
    "](https://towardsdatascience.com/custom-datasets-in-pytorch-part-2-text-machine-translation-71c41a3e994e). This customized function helps with creating padding after batch is created. Each sentence needs to be equal in length in a batch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5697254b-1c00-4131-acc5-c9a0d4322ac0",
   "metadata": {
    "id": "5697254b-1c00-4131-acc5-c9a0d4322ac0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "customed_collate_fn helps with keeping tensors to be equal size for each entry during training.\n",
    "\"\"\"\n",
    "class customed_collate_fn:\n",
    "    def __init__(self, pad_index):\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        src = [item[0] for item in batch]\n",
    "        src = nn.utils.rnn.pad_sequence(src, padding_value=self.pad_index)\n",
    "\n",
    "        tar = [item[1] for item in batch]\n",
    "        tar = nn.utils.rnn.pad_sequence(tar, padding_value=self.pad_index)\n",
    "\n",
    "        return src, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a5c6190-d6d3-4162-ab28-0a76f8eb308d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9a5c6190-d6d3-4162-ab28-0a76f8eb308d",
    "outputId": "1c7a5867-372d-40f3-dbc9-45cb8be6833e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loader done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loss function:\n",
    "    - sums over the negative log likelihoods that the model gives to the correct word at each\n",
    "        position in the output sentence.\n",
    "    - commonly used is Cross-Entropy Loss for the Neural Machine Translation (NMT)\n",
    "\n",
    "Optimizer:\n",
    "    - is the process of adjusting model parameters to reduce model error in each training step.\n",
    "    - model.parameters() returns an iterator over module parameters\n",
    "\n",
    "DataLoader:\n",
    "    - Each iteration returns a batch of train src and train tar, containing batch size\n",
    "\n",
    "\"\"\"\n",
    "pad_index = 1\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "batch_size = 64 # passing sentences as a batch\n",
    "\n",
    "collate_fn = customed_collate_fn(pad_index)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                shuffle=True)\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                shuffle=False)\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                shuffle=False)\n",
    "\n",
    "print(\"data loader done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8118ea5-8692-45c7-8dfc-d164fe028917",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8118ea5-8692-45c7-8dfc-d164fe028917",
    "outputId": "8044f28c-c504-4204-e3f9-80401034f706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_loader type:  <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "\n",
      "---src---\n",
      "tensor([[205, 359,  73,  ...,  43,  21, 525],\n",
      "        [  5,  25, 256,  ..., 211,  33, 290],\n",
      "        [374,   6, 159,  ...,  83, 164, 945],\n",
      "        ...,\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1]])\n",
      "type                   : <class 'torch.Tensor'>\n",
      "shape of the tensorflow: torch.Size([94, 64])\n",
      "type of a tensorflow   : torch.int64\n",
      "\n",
      "---trg---\n",
      "tensor([[196, 355,  72,  ...,  44,  22, 531],\n",
      "        [  5,  24, 248,  ..., 202,  32, 283],\n",
      "        [377,   6, 143,  ..., 235, 160, 949],\n",
      "        ...,\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1],\n",
      "        [  1,   1,   1,  ...,   1,   1,   1]])\n",
      "type                   : <class 'torch.Tensor'>\n",
      "shape of the tensorflow: torch.Size([95, 64])\n",
      "type of a tensorflow   : torch.int64\n"
     ]
    }
   ],
   "source": [
    "# train_data_loader visualization \n",
    "print(\"train_data_loader type: \", type(train_data_loader))\n",
    "\n",
    "print(\"\\n---src---\")\n",
    "src, trg = next(iter(train_data_loader))\n",
    "print(src) # tensor(src length, batch size)\n",
    "print(\"type                   :\", type(src))\n",
    "print(\"shape of the tensorflow:\", src.shape)\n",
    "print(\"type of a tensorflow   :\", src.dtype) \n",
    "\n",
    "print(\"\\n---trg---\")\n",
    "print(trg) # tensor(trg length, batch size)\n",
    "print(\"type                   :\", type(trg))\n",
    "print(\"shape of the tensorflow:\", trg.shape)\n",
    "print(\"type of a tensorflow   :\", trg.dtype) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453694d3-508b-44df-a1d3-c19425009bdd",
   "metadata": {},
   "source": [
    "### Train and Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe1397a0-5d54-4298-b6e4-eee720850cad",
   "metadata": {
    "id": "fe1397a0-5d54-4298-b6e4-eee720850cad"
   },
   "outputs": [],
   "source": [
    "# Train Loop\n",
    "\"\"\"\n",
    "optimizer.zero_grad(): resets the gradients of all optimized to None\n",
    "enumerate: looping with a counter\n",
    "\n",
    "\"\"\"\n",
    "def train_func(model, train_data_loader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (src_tensor,trg_tensor) in enumerate(train_data_loader):\n",
    "            # print(\"\\i:\", i)\n",
    "        optimizer.zero_grad()\n",
    "            # src = train_dataset[X] # word_to_indices\n",
    "        output = model(src_tensor, trg_tensor) # type of src and trg are Float tensor \n",
    "            # print(\"\\ntype                   :\", type(src))\n",
    "            # print(\"shape of the tensorflow:\", src.shape)\n",
    "            # print(\"type of a tensorflow   :\", src.dtype) \n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg_tensor = trg_tensor[1:].view(-1)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss = criterion(output, trg_tensor)\n",
    "        # loss.requires_grad_()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_data_loader)\n",
    "\n",
    "# Evaluation Loop\n",
    "\"\"\"\n",
    "torch.no_grad(): does not calculate gradients to reduce memory consumption and increase speed.\n",
    "enumerate: looping with a counter\n",
    "\n",
    "\"\"\"\n",
    "def evaluate_func(model, valid_data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (src,trg) in enumerate(valid_data_loader):\n",
    "            output = model(src, trg)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        return epoch_loss / len(valid_data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c005fe-35b2-4e7a-90c3-f473428222db",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ff30d53-adb2-44e8-bfdd-f0fd075ccf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                  | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "echo 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████████▏                                                                                                             | 1/10 [00:14<02:09, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  7.622873730129665\n",
      "valid_loss:  7.521840254465739\n",
      "echo 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████▍                                                                                                 | 2/10 [00:27<01:48, 13.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  6.964691744910346\n",
      "valid_loss:  6.35651954015096\n",
      "echo 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████████████████▌                                                                                     | 3/10 [00:40<01:34, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.940347830454509\n",
      "valid_loss:  6.091824372609456\n",
      "echo 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████▊                                                                         | 4/10 [00:53<01:18, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.781805197397868\n",
      "valid_loss:  6.142773151397705\n",
      "echo 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████████████████                                                             | 5/10 [01:07<01:06, 13.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.76226446363661\n",
      "valid_loss:  6.132030646006267\n",
      "echo 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████████████████████▏                                                | 6/10 [01:20<00:53, 13.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.729699664645725\n",
      "valid_loss:  6.130142529805501\n",
      "echo 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████▍                                    | 7/10 [01:34<00:40, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.7051112916734485\n",
      "valid_loss:  6.130520184834798\n",
      "echo 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 8/10 [01:48<00:27, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.706031375461155\n",
      "valid_loss:  6.132638295491536\n",
      "echo 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 9/10 [02:01<00:13, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.691780726114909\n",
      "valid_loss:  6.131899356842041\n",
      "echo 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:15<00:00, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:  5.6710800064934626\n",
      "valid_loss:  6.1359405517578125\n",
      "epoch done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Epoch\n",
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "best_valid_loss = float(\"inf\")\n",
    "trainingEpoch_loss, validationEpoch_loss = [],[]\n",
    "\n",
    "for echo in tqdm(range(n_epochs)):\n",
    "    print(\"echo\", echo)\n",
    "    train_loss = train_func(model,\n",
    "                            train_data_loader,\n",
    "                            optimizer,\n",
    "                            criterion,\n",
    "                            clip)\n",
    "    valid_loss = evaluate_func(model,\n",
    "                              valid_data_loader,\n",
    "                              criterion)\n",
    "    \n",
    "    print(\"train_loss: \", train_loss)\n",
    "    trainingEpoch_loss.append(train_loss)\n",
    "    print(\"valid_loss: \", valid_loss)\n",
    "    validationEpoch_loss.append(valid_loss)\n",
    "\n",
    "    # predicted_sentence = translation(sentence_4, model, train_X_vocab, train_y_vocab, train_dataset)\n",
    "    # print(\"\\n\\npredicted_sentence:\", predicted_sentence)\n",
    "\n",
    "print(\"epoch done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a8173-37cb-4cca-8d09-903a0d2171e6",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "After the model is trained, we need to visualize the loss for both train and validate to see if it's doing good. I also have the same evaluating method as [github project](https://github.com/bentrevett/pytorch-seq2seq/blob/main/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb) to compare the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6cfd398-b80f-4d21-b694-35709fac4ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train and valid loss \n",
    "plt.plot(trainingEpoch_loss, label='train_loss')\n",
    "plt.plot(validationEpoch_loss,label='val_loss')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "175302e5-dbf9-48e1-86cf-ad86cf67b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.948 | Test PPL: 383.116 |\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_func(model, test_data_loader, criterion)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5b5c7-b83a-4d58-b96c-277c0d54bbbd",
   "metadata": {},
   "source": [
    "### Translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dd001d4-a54d-49f0-ad88-2bcb8ea157a8",
   "metadata": {
    "id": "4dd001d4-a54d-49f0-ad88-2bcb8ea157a8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "translate the sentences in test dataset from informal to formal Japanese.\n",
    "\n",
    "    Sentence > Data Preprocessng > Tokenization > Normalization > Encoder > Decoder >\n",
    "    1. Data Preprocessing: clean the sentence data\n",
    "    2. Normalization     : eliminate unhelpful information in sentence.\n",
    "    3. Tokenization      : turn the sentence into tokens\n",
    "    4. Numericalization  : turn tokens into ids then into tensor\n",
    "    5. Encoder           : provides the initial hidden state for Decoder\n",
    "    6. Decoder           : pass tensor into decoder to predict targeted output\n",
    "\n",
    "\"\"\"\n",
    "def translation(source_sentence, model, src_vocab, trg_vocab, dataset, max_length=20):\n",
    "    print(\"\\n'''''''''''''''''''''''''''''''''''''\")\n",
    "    print(\"source sentence:\", source_sentence)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Data Preprocessing\n",
    "        cleaned_sentence = data_cleaning(source_sentence)\n",
    "        # Normalization\n",
    "        preprocessed_sentence = normalization(cleaned_sentence)\n",
    "        # Tokenization + Numericalization\n",
    "        src_token = src_vocab.lookup_indices([sos_token]) + dataset.numericalized(preprocessed_sentence, src_vocab) + src_vocab.lookup_indices([eos_token])\n",
    "        # src_token = src_vocab.lookup_indices([sos_token]) + src_vocab.lookup_indices(tokenization(preprocessed_sentence)) + src_vocab.lookup_indices([eos_token])\n",
    "\n",
    "        print(src_token)\n",
    "        src_display(src_token) # type of src_token = list of int indices \n",
    "\n",
    "        # Encoder\n",
    "        # tensor = torch.LongTensor(src_token).unsqueeze(-1)\n",
    "        tensor = torch.tensor(src_token).unsqueeze(-1)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "\n",
    "        predicted_sentence_index = trg_vocab.lookup_indices([sos_token]) # store indices\n",
    "        # print(\"initial:\", predicted_sentence_index)\n",
    "        predicted_sentence_string = [] # store tokens \n",
    "\n",
    "        # Decoder\n",
    "        for i in range(max_length):\n",
    "            # src_tensor = torch.LongTensor([predicted_sentence_index[-1]])\n",
    "            src_tensor = torch.tensor([predicted_sentence_index[-1]])\n",
    "            output, hidden, cell = model.decoder(src_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            predicted_sentence_index.append(predicted_token)\n",
    "            predicted_sentence_string.append(trg_vocab.get_itos()[predicted_token])\n",
    "\n",
    "            if predicted_token == trg_vocab[eos_token]:\n",
    "                break\n",
    "\n",
    "    return predicted_sentence_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2b16256-6a28-4b38-85cb-1b09762f10d6",
   "metadata": {
    "id": "f2b16256-6a28-4b38-85cb-1b09762f10d6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: JR京都駅に到着した湖西線の列車内に不審物があると、駅員から通報があっだ\n",
      "[2, 0, 0, 2104, 5, 0, 13, 8, 0, 0, 4, 1641, 905, 5, 0, 337, 9, 42, 11, 0, 23, 0, 9, 153, 27, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> <unk> 駅 に <unk> し た <unk> <unk> の 列車 内 に <unk> 物 が ある と <unk> から <unk> が あっ だ <eos> \n",
      "predicted_sentence: ['は', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'て', 'た', 'た', 'た', 'た', 'た', 'た', 'た']\n",
      "\n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 京都駅を発着する列車の運転を1時間半くらいにわたって見合わせた\n",
      "[2, 0, 2104, 10, 0, 29, 1641, 4, 1249, 10, 32, 133, 0, 287, 5, 0, 7, 0, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> 駅 を <unk> する 列車 の 運転 を 0 時間 <unk> くらい に <unk> て <unk> た <eos> \n",
      "predicted_sentence: ['は', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'て', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た']\n",
      "\n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: でもそのイベントの前からチッソ株式会社は継続的に汚染していた\n",
      "[2, 12, 14, 48, 1467, 4, 118, 23, 514, 591, 162, 6, 1969, 113, 5, 1877, 13, 7, 24, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> で も その イベント の 前 から チッソ 株式 会社 は 継続 的 に 汚染 し て い た <eos> \n",
      "predicted_sentence: ['は', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'て', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た']\n",
      "\n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 週末街に出た\n",
      "[2, 1248, 2003, 5, 391, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 週末 街 に 出 た <eos> \n",
      "predicted_sentence: ['の', 'の', 'の', 'て', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た']\n",
      "\n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 週末\n",
      "[2, 1248, 3]\n",
      "\n",
      "src (indice -> word): <sos> 週末 <eos> \n",
      "predicted_sentence: ['の', 'の', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た', 'た']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Evaluation: testing from informal to formal Japanese\n",
    "'''\n",
    "sentence = \"JR京都駅に到着した湖西線の列車内に不審物があると、駅員から通報があっだ\"\n",
    "sentence_2 = \"京都駅を発着する列車の運転を1時間半くらいにわたって見合わせた\"\n",
    "sentence_3 = \"でもそのイベントの前からチッソ株式会社は継続的に汚染していた\"\n",
    "sentence_4 = \"週末街に出た\"\n",
    "sentence_4_trg = \"週末街に出てきました\"\n",
    "sentence_5 = \"週末\"\n",
    "\n",
    "predicted_sentence = translation(sentence, model, train_X_vocab, train_y_vocab, train_dataset)\n",
    "print(\"\\npredicted_sentence:\", predicted_sentence)\n",
    "\n",
    "predicted_sentence = translation(sentence_2, model, train_X_vocab, train_y_vocab, train_dataset)\n",
    "print(\"\\npredicted_sentence:\", predicted_sentence)\n",
    "\n",
    "predicted_sentence = translation(sentence_3, model, train_X_vocab, train_y_vocab, train_dataset)\n",
    "print(\"\\npredicted_sentence:\", predicted_sentence)\n",
    "\n",
    "predicted_sentence = translation(sentence_4, model, train_X_vocab, train_y_vocab, train_dataset)\n",
    "print(\"\\npredicted_sentence:\", predicted_sentence)\n",
    "\n",
    "predicted_sentence = translation(sentence_5, model, train_X_vocab, train_y_vocab, train_dataset)\n",
    "print(\"\\npredicted_sentence:\", predicted_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "pUq-t6QMblH7",
   "metadata": {
    "id": "pUq-t6QMblH7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 今日は午前９時に大学へ行こうとしたが、家の近くの道路が、工事中のため通行止めでした。\n",
      "[2, 97, 6, 1656, 32, 49, 5, 263, 141, 623, 11, 13, 8, 9, 186, 4, 2049, 4, 1252, 9, 1749, 45, 4, 82, 2060, 1863, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 今日 は 午前 0 時 に 大学 へ 行こう と し た が 家 の 近く の 道路 が 工事 中 の ため 通行 止め でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ちなみに、台湾の人気ウェブは\"台灣論壇\"や\"台大PTT(BBS)\"です。\n",
      "[2, 363, 5, 546, 4, 148, 781, 6, 0, 0, 65, 940, 220, 0, 105, 0, 106, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> ちなみ に 台湾 の 人気 ウェブ は <unk> <unk> や 台 大 <unk> ( <unk> ) です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ちなみに、台湾の人気ウェブは\"台灣論壇\"や\"台大PTT(BBS)\"です。\n",
      "[2, 363, 5, 546, 4, 148, 781, 6, 0, 0, 65, 940, 220, 0, 105, 0, 106, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> ちなみ に 台湾 の 人気 ウェブ は <unk> <unk> や 台 大 <unk> ( <unk> ) です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: それで、中国政府は世論の圧力に対して日本側に何かしないといけません。\n",
      "[2, 44, 12, 53, 167, 6, 1564, 4, 552, 5, 223, 7, 21, 218, 5, 79, 20, 13, 18, 11, 238, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> それ で 中国 政府 は 世論 の 圧力 に 対し て 日本 側 に 何 か し ない と いけ ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 主賓とご両親はスピーチをして、司会者はお客さんを楽しませて、コンクールを行いました。\n",
      "[2, 0, 11, 698, 1565, 6, 1492, 10, 13, 7, 942, 126, 6, 56, 992, 34, 10, 0, 128, 7, 798, 10, 0, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> と ご 両親 は スピーチ を し て 司会 者 は お 客 さん を <unk> せ て コンクール を <unk> まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本の一般人と日本語で喋れると思うけど、大学の授業の場合は聞き取れないに違いないです。\n",
      "[2, 21, 4, 1554, 26, 11, 21, 33, 12, 1686, 11, 166, 39, 263, 4, 151, 4, 964, 6, 1981, 18, 5, 233, 18, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 の 一般 人 と 日本 語 で 喋れる と 思う けど 大学 の 授業 の 場合 は 聞き取れ ない に 違い ない です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 心の中では敬服はしていたが、でも、畢竟よい兆しではない。だが、人は其々で好き好きなので、無理やりすることが出来まいと思っていた。\n",
      "[2, 419, 4, 45, 12, 6, 1820, 6, 13, 7, 24, 8, 9, 12, 14, 1929, 1453, 894, 12, 6, 18, 27, 9, 26, 6, 1628, 12, 1719, 19, 4, 12, 1901, 29, 22, 9, 310, 1436, 11, 132, 7, 24, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 心 の 中 で は 敬服 は し て い た が で も 畢竟 よい 兆し で は ない だ が 人 は 其 で 好き好き な の で 無理やり する こと が 出来 まい と 思っ て い た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 一人ぼっちになった私が教室で、もうどうにも我慢できなくて、大声で泣き出しました。\n",
      "[2, 1546, 5, 55, 8, 25, 9, 1818, 12, 142, 64, 5, 14, 579, 57, 52, 7, 973, 12, 1880, 260, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 一人ぼっち に なっ た 私 が 教室 で もう どう に も 我慢 でき なく て 大声 で 泣き 出し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: やだよ！　自分で渡せよ\n",
      "[2, 65, 27, 109, 59, 12, 0, 3]\n",
      "\n",
      "src (indice -> word): <sos> や だ よ 自分 で <unk> <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 牡丹は葉が落ちて黒く細い枝が枯れているように見えるが、どういう意味があるだろう？\n",
      "[2, 194, 6, 1998, 9, 1997, 7, 2110, 1964, 191, 9, 1850, 7, 30, 40, 5, 1201, 9, 64, 31, 226, 9, 42, 129, 3]\n",
      "\n",
      "src (indice -> word): <sos> 牡丹 は 葉 が 落ち て 黒く 細い 枝 が 枯れ て いる よう に 見える が どう いう 意味 が ある だろう <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 主賓とご両親はスピーチをして、司会者はお客さんを楽しませて、コンクールを行いました。\n",
      "[2, 0, 11, 698, 1565, 6, 1492, 10, 13, 7, 942, 126, 6, 56, 992, 34, 10, 0, 128, 7, 798, 10, 0, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> と ご 両親 は スピーチ を し て 司会 者 は お 客 さん を <unk> せ て コンクール を <unk> まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 彼女が日本語を学ぶ切っ掛けは高校２年生の時の授業のためでした。\n",
      "[2, 120, 9, 21, 33, 10, 1725, 1639, 6, 353, 32, 110, 1920, 4, 49, 4, 151, 4, 82, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 彼女 が 日本 語 を 学ぶ 切っ掛け は 高校 0 年 生 の 時 の 授業 の ため でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 初日はロシアの伝統に従って、花婿は親友と一緒に花嫁を身受けしなければなりました。\n",
      "[2, 1642, 6, 384, 4, 149, 5, 1770, 7, 621, 6, 345, 11, 102, 5, 454, 10, 2037, 13, 116, 66, 87, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 初日 は ロシア の 伝統 に 従っ て 花婿 は 親友 と 一緒 に 花嫁 を 身受け し なけれ ば なり まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: あまりメールなどを書かないので敬語の使い方はかなり不安になりました。\n",
      "[2, 285, 1529, 130, 10, 1835, 18, 4, 12, 1821, 4, 886, 112, 6, 1365, 302, 5, 87, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> あまり メール など を 書か ない の で 敬語 の 使い 方 は かなり 不安 に なり まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: そして、日本のメディアは船長の裏には中国政府の仕業かもしれないとも書いています。\n",
      "[2, 73, 21, 4, 838, 6, 619, 4, 2006, 5, 6, 53, 167, 4, 1591, 20, 14, 99, 18, 11, 14, 125, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> そして 日本 の メディア は 船長 の 裏 に は 中国 政府 の 仕業 か も しれ ない と も 書い て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 学生の時といわず、社会人の時といわず、私は穏やかで気品のある話し方をする人であることが変わりません。\n",
      "[2, 185, 4, 49, 11, 0, 127, 135, 26, 4, 49, 11, 0, 127, 25, 6, 0, 12, 0, 4, 42, 461, 112, 10, 29, 26, 12, 42, 22, 9, 0, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 学生 の 時 と <unk> ず 社会 人 の 時 と <unk> ず 私 は <unk> で <unk> の ある 話し 方 を する 人 で ある こと が <unk> ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私たちは　五日間　過ごした。　最初の三日間は　雨が　非常に　降って　大変でした。\n",
      "[2, 25, 76, 6, 1583, 1067, 348, 8, 228, 4, 858, 1067, 6, 0, 9, 2094, 5, 0, 7, 408, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 たち は 五 日間 過ごし た 最初 の 三 日間 は <unk> が 非常 に <unk> て 大変 でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: このストーリーは､実らない恋についてのちょっと悲しいストーリーです。\n",
      "[2, 43, 147, 6, 1737, 18, 574, 5, 123, 7, 4, 69, 420, 147, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> この ストーリー は 実ら ない 恋 に つい て の ちょっと 悲しい ストーリー です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: そして、夢中で時間を過ごし、現実世界に帰る時が来た。\n",
      "[2, 73, 0, 12, 133, 10, 348, 0, 303, 5, 1754, 49, 9, 433, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> そして <unk> で 時間 を 過ごし <unk> 世界 に 帰る 時 が 来 た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: その時のことがなければ、今の私はキャノン杯に参加する勇気まで失ってしまうでしょう。\n",
      "[2, 48, 49, 4, 22, 9, 116, 66, 78, 4, 25, 6, 790, 1080, 5, 395, 29, 539, 94, 558, 7, 1382, 140, 3]\n",
      "\n",
      "src (indice -> word): <sos> その 時 の こと が なけれ ば 今 の 私 は キャノン 杯 に 参加 する 勇気 まで 失っ て しまう でしょう <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 中国の土地は広いから、たくさんの人が実家から遠い場所で働いて、家族と離れています。\n",
      "[2, 53, 4, 1693, 6, 1758, 23, 92, 4, 26, 9, 563, 23, 2070, 553, 12, 1619, 7, 165, 11, 2086, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 中国 の 土地 は 広い から たくさん の 人 が 実家 から 遠い 場所 で 働い て 家族 と 離れ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 地下鉄に乗る時、お年寄りとか、中年の人とか、若者とか、学生とか、誰でもカバンを持っていることだった。\n",
      "[2, 0, 0, 5, 869, 49, 56, 0, 11, 20, 0, 4, 26, 11, 20, 114, 11, 20, 185, 11, 20, 1225, 12, 14, 504, 10, 111, 7, 30, 22, 83, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> <unk> に 乗る 時 お <unk> と か <unk> の 人 と か 若者 と か 学生 と か 誰 で も カバン を 持っ て いる こと だっ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: カバンを持っている人はたぶんいろいろなものを入れておくと便利だと考える節があるが、もう一つは若い者は衣服を飾ることだと思う。\n",
      "[2, 504, 10, 111, 7, 30, 26, 6, 289, 201, 19, 77, 10, 0, 7, 0, 11, 0, 27, 11, 0, 0, 9, 42, 9, 142, 67, 208, 6, 1190, 126, 6, 0, 10, 0, 22, 27, 11, 166, 3]\n",
      "\n",
      "src (indice -> word): <sos> カバン を 持っ て いる 人 は たぶん いろいろ な もの を <unk> て <unk> と <unk> だ と <unk> <unk> が ある が もう 一 つ は 若い 者 は <unk> を <unk> こと だ と 思う <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: その公園の中で、多くのアトラクションと乗り物を楽しむことができるので、オーストラリアの中で、その公園は一番すごいそうです。\n",
      "[2, 48, 219, 4, 45, 12, 1710, 4, 301, 11, 1574, 10, 0, 22, 9, 107, 4, 12, 0, 4, 45, 12, 48, 219, 6, 71, 708, 81, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> その 公園 の 中 で 多く の アトラクション と 乗り物 を <unk> こと が できる の で <unk> の 中 で その 公園 は 一番 すごい そう です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ちなみに、日本の部屋のレンタル費がすごく高いから..........\n",
      "[2, 363, 5, 21, 4, 0, 4, 0, 2030, 9, 243, 2107, 23, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 3]\n",
      "\n",
      "src (indice -> word): <sos> ちなみ に 日本 の <unk> の <unk> 費 が すごく 高い から . . . . . . . . . . <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: しかし、日本側は選挙が終わっても船長を釈放する様子が全然ありませんから、やっと本気で抗議し始めます。\n",
      "[2, 1380, 21, 218, 6, 631, 9, 1169, 7, 14, 619, 10, 2071, 29, 1089, 9, 163, 36, 35, 28, 23, 763, 1842, 12, 422, 13, 412, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> しかし 日本 側 は 選挙 が 終わっ て も 船長 を 釈放 する 様子 が 全然 あり ませ ん から やっと 本気 で 抗議 し 始め ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ヤンキーのリーダーはせんぱいで、こうはいはせんぱいに話す時に、敬語を使わなければならない。\n",
      "[2, 180, 4, 1537, 6, 0, 12, 0, 6, 0, 5, 2020, 49, 5, 1821, 10, 0, 116, 66, 108, 18, 3]\n",
      "\n",
      "src (indice -> word): <sos> ヤンキー の リーダー は <unk> で <unk> は <unk> に 話す 時 に 敬語 を <unk> なけれ ば なら ない <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 西洋人にとって、愛情はすべて！」......先生はそう言ったけど、実際の状況は本当にそうなのか。\n",
      "[2, 1199, 26, 5, 100, 7, 0, 6, 1388, 37, 37, 37, 37, 37, 37, 84, 6, 81, 138, 8, 39, 0, 4, 1912, 6, 152, 5, 81, 19, 4, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> 西洋 人 に とっ て <unk> は すべて . . . . . . 先生 は そう 言っ た けど <unk> の 状況 は 本当 に そう な の か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 時々、アメリカの女は”あたしは優しいガイが欲しい”と言ってんだけど、これらの人はいつも”優しいガイ”を拒否するか無視します。\n",
      "[2, 49, 89, 4, 1718, 6, 234, 476, 6, 892, 788, 9, 1859, 234, 11, 138, 7, 28, 27, 39, 241, 300, 4, 26, 6, 91, 14, 234, 892, 788, 234, 10, 1802, 29, 20, 1902, 13, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 時 アメリカ の 女 は \" あたし は 優しい ガイ が 欲しい \" と 言っ て ん だ けど これ ら の 人 は いつ も \" 優しい ガイ \" を 拒否 する か 無視 し ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 相手をどんなに愛しても､自分が相手にどんなに愛されても､恋人になったら､いつか別れると彼女はそう信じます。\n",
      "[2, 445, 10, 247, 5, 577, 7, 14, 59, 9, 445, 5, 247, 5, 1790, 41, 7, 14, 1780, 5, 55, 38, 91, 20, 1645, 11, 120, 6, 81, 182, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 相手 を どんな に 愛し て も 自分 が 相手 に どんな に 愛さ れ て も 恋人 に なっ たら いつ か 別れる と 彼女 は そう 信じ ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: なぜかよく分からないが、日本人とスカイプコールをするたびに、通信が悪くなる。\n",
      "[2, 1414, 20, 70, 392, 18, 9, 21, 26, 11, 803, 1480, 10, 29, 719, 5, 2058, 9, 1785, 101, 3]\n",
      "\n",
      "src (indice -> word): <sos> なぜ か よく 分から ない が 日本 人 と スカイプ コール を する たび に 通信 が 悪く なる <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: もし赤ちゃんがあきらめたら、今でも歩くことができないと思います。\n",
      "[2, 143, 347, 9, 1338, 38, 78, 12, 14, 594, 22, 9, 57, 18, 11, 54, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> もし 赤ちゃん が あきらめ たら 今 で も 歩く こと が でき ない と 思い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: （）のところに①と②、どちらを入れるのが一番よいのでしょうか？\n",
      "[2, 4, 156, 5, 11, 0, 10, 0, 4, 9, 71, 1453, 4, 140, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> の ところ に と <unk> を <unk> の が 一番 よい の でしょう か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: その瞬間に、彼は本当に幅広く感情を引き起こせるということに気づいた。\n",
      "[2, 48, 0, 5, 188, 6, 152, 5, 0, 0, 10, 0, 11, 31, 22, 5, 0, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> その <unk> に 彼 は 本当 に <unk> <unk> を <unk> と いう こと に <unk> た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: （原作は東野圭吾の同名の小説です。）私はすごく感動して、最後までいっぱい泣いちゃいました。\n",
      "[2, 312, 6, 1847, 1696, 4, 1674, 4, 566, 15, 25, 6, 243, 578, 13, 7, 190, 94, 480, 1879, 493, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 原作 は 東野 圭吾 の 同名 の 小説 です 私 は すごく 感動 し て 最後 まで いっぱい 泣い ちゃい まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 特に最後の元気そうなお母ちゃんとその子供の笑顔に心が温まった。\n",
      "[2, 440, 190, 4, 893, 81, 19, 56, 272, 244, 11, 48, 316, 4, 1959, 5, 419, 9, 1892, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 特に 最後 の 元気 そう な お 母 ちゃん と その 子供 の 笑顔 に 心 が 温まっ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本語より、英語と中国語の言葉を排列する順番は似ているので（英語が苦手なんですけど）、中高生のころだったんですが、英語を中国語に訳すことがあまり厄介ではなかった気がしました。\n",
      "[2, 21, 33, 157, 278, 11, 53, 33, 4, 80, 10, 1807, 29, 641, 6, 1606, 7, 30, 4, 12, 278, 9, 1992, 19, 28, 15, 39, 1570, 4, 1376, 83, 8, 28, 15, 9, 278, 10, 53, 33, 5, 2018, 22, 9, 285, 1662, 12, 6, 62, 8, 98, 9, 13, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 語 より 英語 と 中国 語 の 言葉 を 排列 する 順番 は 似 て いる の で 英語 が 苦手 な ん です けど 中高生 の ころ だっ た ん です が 英語 を 中国 語 に 訳す こと が あまり 厄介 で は なかっ た 気 が し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: なんだか中国語らしくないせいか、なんとなく意味はわかりますけど、でも頭にとどまっていられなく、すぐ忘れてしまう文章だと思いました。\n",
      "[2, 292, 27, 20, 53, 33, 1456, 18, 488, 20, 292, 11, 52, 226, 6, 0, 17, 39, 12, 14, 352, 5, 0, 7, 24, 96, 52, 205, 572, 7, 1382, 587, 27, 11, 54, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> なん だ か 中国 語 らしく ない せい か なん と なく 意味 は <unk> ます けど で も 頭 に <unk> て い られ なく すぐ 忘れ て しまう 文章 だ と 思い まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本の教育制度の究極の目的は日本人は社会に入って、社会人になる。\n",
      "[2, 21, 4, 267, 536, 4, 1956, 4, 444, 6, 21, 26, 6, 135, 5, 307, 7, 135, 26, 5, 101, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 の 教育 制度 の 究極 の 目的 は 日本 人 は 社会 に 入っ て 社会 人 に なる <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 子供の時から今まで、私はよく責任を引き受けて、真面目な人になるように育てられています。\n",
      "[2, 316, 4, 49, 23, 78, 94, 25, 6, 70, 1231, 10, 0, 7, 0, 19, 26, 5, 101, 40, 5, 0, 96, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 子供 の 時 から 今 まで 私 は よく 責任 を <unk> て <unk> な 人 に なる よう に <unk> られ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: たとえば、今日本の若者文化のシンボルと言えば、私はなんとなく「ロリータファッション」を頭に浮かべました。\n",
      "[2, 362, 78, 21, 4, 114, 60, 4, 1487, 11, 2015, 66, 25, 6, 292, 11, 52, 385, 179, 10, 352, 5, 1887, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> たとえば 今 日本 の 若者 文化 の シンボル と 言え ば 私 は なん と なく ロリータ ファッション を 頭 に 浮かべ まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 明日は日本からの友たちにこの町を案内してあげるから、今日わたしと代わる人を見つけなきゃいけない。\n",
      "[2, 268, 6, 21, 23, 4, 0, 76, 5, 43, 1132, 10, 0, 13, 7, 0, 23, 97, 503, 11, 0, 26, 10, 624, 0, 238, 18, 3]\n",
      "\n",
      "src (indice -> word): <sos> 明日 は 日本 から の <unk> たち に この 町 を <unk> し て <unk> から 今日 わたし と <unk> 人 を 見つけ <unk> いけ ない <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 先生は刘伯温からもらった紺の菊花を手に持ってみたところ、花の色もいいし、咲き遅れというものの、やはり咲いたものなので、もしかしたら刘伯温が大器晩成となる（のし上がるのが遅い）という兆しかもしれない。\n",
      "[2, 84, 6, 915, 1113, 23, 248, 8, 1965, 4, 1995, 10, 325, 5, 111, 7, 95, 8, 156, 85, 4, 620, 14, 50, 13, 550, 2066, 11, 31, 77, 4, 499, 1682, 8, 77, 19, 4, 12, 143, 20, 13, 38, 915, 1113, 9, 1714, 1828, 11, 101, 4, 1385, 4, 9, 2065, 11, 31, 894, 20, 14, 99, 18, 3]\n",
      "\n",
      "src (indice -> word): <sos> 先生 は 刘伯 温 から もらっ た 紺 の 菊花 を 手 に 持っ て み た ところ 花 の 色 も いい し 咲き 遅れ と いう もの の やはり 咲い た もの な の で もし か し たら 刘伯 温 が 大器 晩成 と なる の し上がる の が 遅い と いう 兆し か も しれ ない <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本のみへのレアアース輸出の禁止（ＷＴＯ違反）や、日本関係の通関手続きの妨害とかもしていた原因でもあると思います。\n",
      "[2, 21, 1418, 141, 4, 1540, 1464, 2044, 4, 1154, 1335, 2069, 65, 21, 470, 4, 2061, 1800, 4, 1720, 11, 20, 14, 13, 7, 24, 8, 394, 12, 14, 42, 11, 54, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 のみ へ の レア アース 輸出 の 禁止 wto 違反 や 日本 関係 の 通関 手続き の 妨害 と か も し て い た 原因 で も ある と 思い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 僕にとってそういう勉強する方法で、言葉を完璧に覚えられるわけではないけれど、ANKIのおかげで言葉の意味の種を僕の頭に植えられる。\n",
      "[2, 131, 5, 100, 7, 81, 31, 119, 29, 1825, 12, 80, 10, 0, 5, 458, 0, 252, 12, 6, 18, 203, 0, 4, 673, 12, 80, 4, 226, 4, 0, 10, 131, 4, 352, 5, 0, 0, 3]\n",
      "\n",
      "src (indice -> word): <sos> 僕 に とっ て そう いう 勉強 する 方法 で 言葉 を <unk> に 覚え <unk> わけ で は ない けれど <unk> の おかげ で 言葉 の 意味 の <unk> を 僕 の 頭 に <unk> <unk> <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 新婚夫婦はほとんど見知らない人である遠い親戚を招待しなければなりません。\n",
      "[2, 424, 979, 6, 295, 0, 18, 26, 12, 42, 2070, 0, 10, 1043, 13, 116, 66, 87, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 新婚 夫婦 は ほとんど <unk> ない 人 で ある 遠い <unk> を 招待 し なけれ ば なり ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 皆さんはThomas Alfa Edison（トーマスアルファエジソン）をしっていますか。\n",
      "[2, 90, 34, 6, 1328, 1295, 235, 1497, 10, 1381, 7, 24, 17, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> 皆 さん は thomas alfa edison トーマスアルファエジソン を しっ て い ます か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 「いいアイデアが一つあったら、金持ちになれますよ」と言った人がいる。\n",
      "[2, 50, 0, 9, 67, 208, 153, 38, 350, 5, 734, 17, 109, 11, 138, 8, 26, 9, 30, 3]\n",
      "\n",
      "src (indice -> word): <sos> いい <unk> が 一 つ あっ たら 金持ち に なれ ます よ と 言っ た 人 が いる <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ケーキもあれば、ビールもありました。皆、酒に酔うとゲームをしていました。有名なゲーム、[本当な話、大冒険]、他人の内緒を知りたいことは皆の興味ですね。\n",
      "[2, 0, 14, 1344, 66, 380, 14, 36, 16, 8, 90, 0, 5, 0, 11, 216, 10, 13, 7, 24, 16, 8, 431, 19, 216, 0, 152, 19, 171, 220, 909, 0, 872, 4, 0, 10, 0, 63, 22, 6, 90, 4, 1184, 15, 88, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> も あれ ば ビール も あり まし た 皆 <unk> に <unk> と ゲーム を し て い まし た 有名 な ゲーム <unk> 本当 な 話 大 冒険 <unk> 他人 の <unk> を <unk> たい こと は 皆 の 興味 です ね <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 自分を守るために､知らず知らずのうちに別のいいこと(大切な人とか)を失っているかもしれません。\n",
      "[2, 59, 10, 989, 82, 5, 1146, 127, 1146, 127, 4, 154, 5, 917, 4, 50, 22, 105, 184, 19, 26, 11, 20, 106, 10, 558, 7, 30, 20, 14, 99, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 自分 を 守る ため に 知ら ず 知ら ず の うち に 別 の いい こと ( 大切 な 人 と か ) を 失っ て いる か も しれ ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私は高校三年生だ、もうすぐ大学入学試験があるから、毎日勉強しなければならない、大変です！\n",
      "[2, 25, 6, 353, 858, 110, 1920, 27, 142, 205, 263, 0, 460, 9, 42, 23, 230, 119, 13, 116, 66, 108, 18, 408, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 は 高校 三 年 生 だ もう すぐ 大学 <unk> 試験 が ある から 毎日 勉強 し なけれ ば なら ない 大変 です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 他の東南アジア諸国は元々中国軍に対して危機感をつのらせており、近年の経済発展に伴う東南アジア全体の軍備増強は目覚しいものがあると聞いています。\n",
      "[2, 1592, 4, 1082, 774, 2027, 6, 1621, 53, 2040, 5, 223, 7, 1660, 1031, 10, 1405, 128, 7, 676, 2050, 4, 1172, 1938, 5, 1602, 1082, 774, 1626, 4, 2042, 1703, 6, 1942, 77, 9, 42, 11, 136, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 他 の 東南 アジア 諸国 は 元 中国 軍 に 対し て 危機 感 を つのら せ て おり 近年 の 経済 発展 に 伴う 東南 アジア 全体 の 軍備 増強 は 目覚しい もの が ある と 聞い て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 対応のため、約15分遅延した。\n",
      "[2, 0, 4, 82, 342, 32, 1635, 0, 13, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> の ため 約 0 分 <unk> し た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 小南さん(古天樂さんの役です)と小米さん二人は大学時代から､ずっとお互いに愛しているけど､結末は二人は一緒になりませんでした。\n",
      "[2, 1001, 34, 105, 937, 410, 436, 34, 4, 1019, 15, 106, 11, 319, 34, 161, 6, 263, 427, 23, 206, 56, 0, 5, 577, 7, 30, 39, 0, 6, 161, 6, 102, 5, 87, 35, 28, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 小南 さん ( 古 天 樂 さん の 役 です ) と 小米 さん 二人 は 大学 時代 から ずっと お <unk> に 愛し て いる けど <unk> は 二人 は 一緒 に なり ませ ん でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 国内のメディアも管理されて過激な言論はいっさい禁止されています。\n",
      "[2, 262, 4, 838, 14, 613, 51, 41, 7, 1250, 19, 2016, 6, 1349, 1154, 51, 41, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 国内 の メディア も 管理 さ れ て 過激 な 言論 は いっさい 禁止 さ れ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 海で泳ぐのは最高！波に乗ってサーフィンもしたい！\n",
      "[2, 1106, 12, 1883, 4, 6, 1837, 0, 5, 868, 7, 0, 14, 13, 63, 3]\n",
      "\n",
      "src (indice -> word): <sos> 海 で 泳ぐ の は 最高 <unk> に 乗っ て <unk> も し たい <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: この週末はアメリカにいる最後だから、私の友達と家族と遊んでいるんですが、時間が止まったようです。\n",
      "[2, 43, 1248, 6, 89, 5, 30, 190, 27, 23, 25, 4, 72, 11, 165, 11, 629, 12, 30, 28, 15, 9, 133, 9, 1862, 8, 40, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> この 週末 は アメリカ に いる 最後 だ から 私 の 友達 と 家族 と 遊ん で いる ん です が 時間 が 止まっ た よう です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本に未発売の海外のコスメをサンフランシスコに行く予定があるので、教えて下さい。\n",
      "[2, 21, 5, 0, 0, 4, 1107, 4, 0, 10, 0, 5, 279, 0, 9, 42, 4, 12, 1817, 7, 0, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 に <unk> <unk> の 海外 の <unk> を <unk> に 行く <unk> が ある の で 教え て <unk> <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 王天亮は耳にさらさらと風の吹く音をさせながら疾風迅雷のような速さであっという間に、うちに戻った。\n",
      "[2, 605, 410, 871, 6, 1177, 5, 1378, 11, 471, 4, 1680, 639, 10, 51, 128, 291, 1936, 2047, 4, 40, 19, 2062, 51, 12, 153, 11, 31, 469, 5, 154, 5, 580, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 王 天 亮 は 耳 に さらさら と 風 の 吹く 音 を さ せ ながら 疾風 迅雷 の よう な 速 さ で あっ と いう 間 に うち に 戻っ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: そして、ネット上であの船長は工作員だとかの見方も出てきました。\n",
      "[2, 73, 256, 159, 12, 478, 619, 6, 1750, 955, 27, 11, 20, 4, 104, 112, 14, 391, 7, 86, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> そして ネット 上 で あの 船長 は 工作 員 だ と か の 見 方 も 出 て き まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本人にとって何が面白い？ with Japanese what is funny ?\n",
      "[2, 21, 26, 5, 100, 7, 79, 9, 2095, 661, 236, 1331, 1312, 1307, 1292, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 人 に とっ て 何 が 面白い with japanese what is funny ? <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: それも、かたにはまらないスタイルをもって、オートバイを運転する。\n",
      "[2, 44, 14, 1364, 5, 1422, 18, 511, 10, 1446, 7, 785, 10, 1249, 29, 3]\n",
      "\n",
      "src (indice -> word): <sos> それ も かた に はまら ない スタイル を もっ て オートバイ を 運転 する <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: だから私は Manga Moods と呼ばれる別の本を借りました。\n",
      "[2, 27, 23, 25, 6, 1318, 1319, 11, 952, 214, 917, 4, 134, 10, 1617, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> だ から 私 は manga moods と 呼ば れる 別 の 本 を 借り まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 実はね、私も一年ぐらい日本語の勉強をしました。でも、あんまりできません。\n",
      "[2, 318, 6, 88, 25, 14, 67, 110, 484, 21, 33, 4, 119, 10, 13, 16, 8, 12, 14, 0, 57, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 実 は ね 私 も 一 年 ぐらい 日本 語 の 勉強 を し まし た で も <unk> でき ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ここで気をつけなければならないところは日本の法律はアメリカと同じです。今度の事件は日本国内法の手順で完全に裁判されました。\n",
      "[2, 240, 12, 98, 10, 0, 116, 66, 108, 18, 156, 6, 21, 4, 0, 6, 89, 11, 398, 15, 0, 4, 1580, 6, 21, 262, 1103, 4, 0, 12, 1731, 5, 344, 51, 41, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> ここ で 気 を <unk> なけれ ば なら ない ところ は 日本 の <unk> は アメリカ と 同じ です <unk> の 事件 は 日本 国内 法 の <unk> で 完全 に 裁判 さ れ まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 軍事行為を諦めない通告が報道された後、もう一つの日本の国内ニュースを考えてください。\n",
      "[2, 2041, 2002, 10, 2026, 18, 2059, 9, 1700, 51, 41, 8, 103, 142, 67, 208, 4, 21, 4, 262, 515, 10, 121, 7, 202, 3]\n",
      "\n",
      "src (indice -> word): <sos> 軍事 行為 を 諦め ない 通告 が 報道 さ れ た 後 もう 一 つ の 日本 の 国内 ニュース を 考え て ください <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: メルボルンから飛行機で約二時半間の名所にあるゴールドコーストは、有名で色々なテーマパークで遊べる所です。\n",
      "[2, 519, 23, 472, 270, 12, 342, 160, 49, 1658, 4, 948, 5, 42, 508, 6, 431, 12, 620, 19, 815, 1509, 12, 2067, 324, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> メルボルン から 飛行 機 で 約 二 時 半間 の 名所 に ある ゴールドコースト は 有名 で 色 な テーマ パーク で 遊べる 所 です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 一番前に座っている学生が親切にこっそりその歌詞を教えてくれましたが、全然うまく歌えませんでした。\n",
      "[2, 71, 118, 5, 1761, 7, 30, 185, 9, 1205, 5, 1375, 48, 592, 10, 1817, 7, 173, 16, 8, 9, 163, 1353, 1860, 35, 28, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 一番 前 に 座っ て いる 学生 が 親切 に こっそり その 歌詞 を 教え て くれ まし た が 全然 うまく 歌え ませ ん でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 新婚夫婦はほとんど見知らない人である遠い親戚を招待しなければなりません。\n",
      "[2, 424, 979, 6, 295, 0, 18, 26, 12, 42, 2070, 0, 10, 1043, 13, 116, 66, 87, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 新婚 夫婦 は ほとんど <unk> ない 人 で ある 遠い <unk> を 招待 し なけれ ば なり ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 「菅首相が小沢氏に圧勝 721対491」(2010.09.14)\n",
      "[2, 0, 0, 9, 0, 0, 5, 0, 32, 0, 32, 105, 32, 37, 32, 37, 32, 106, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> <unk> が <unk> <unk> に <unk> 0 <unk> 0 ( 0 . 0 . 0 ) <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 旅行している時に迷ったら........北海道はいつもきれいなところだと思います。\n",
      "[2, 168, 13, 7, 30, 49, 5, 2052, 38, 37, 37, 37, 37, 37, 37, 37, 37, 542, 6, 91, 14, 1372, 19, 156, 27, 11, 54, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 旅行 し て いる 時 に 迷っ たら . . . . . . . . 北海道 は いつ も きれい な ところ だ と 思い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本に未発売の海外のコスメをサンフランシスコに行く予定があるので、教えて下さい。\n",
      "[2, 21, 5, 0, 0, 4, 1107, 4, 0, 10, 0, 5, 279, 0, 9, 42, 4, 12, 1817, 7, 0, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 に <unk> <unk> の 海外 の <unk> を <unk> に 行く <unk> が ある の で 教え て <unk> <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: それに、足の内部に痺れがあるので、水俣病を持っている人は歩けなかった。\n",
      "[2, 44, 5, 2035, 4, 1629, 5, 1138, 9, 42, 4, 12, 231, 170, 10, 111, 7, 30, 26, 6, 1865, 62, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> それ に 足 の 内部 に 痺れ が ある の で 水俣 病 を 持っ て いる 人 は 歩け なかっ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: その時、たろうは心の中でつぶやいた。「本当の冒険は、想像を超えるものなんだな。」\n",
      "[2, 48, 49, 207, 6, 419, 4, 45, 12, 0, 8, 152, 4, 909, 6, 1788, 10, 0, 77, 19, 28, 27, 19, 3]\n",
      "\n",
      "src (indice -> word): <sos> その 時 たろう は 心 の 中 で <unk> た 本当 の 冒険 は 想像 を <unk> もの な ん だ な <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 残念ながら、数年ほど前に最後の一匹は老齢のため亡くなりました。\n",
      "[2, 1867, 291, 327, 110, 177, 118, 5, 190, 4, 67, 393, 6, 1977, 4, 82, 1584, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 残念 ながら 数 年 ほど 前 に 最後 の 一 匹 は 老齢 の ため 亡くなり まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: それだけならまだいいですけど、一番怖いのはコンクールの日は学校の全員の前で歌わなければなりません。\n",
      "[2, 44, 61, 108, 117, 50, 15, 39, 71, 0, 4, 6, 798, 4, 47, 6, 317, 4, 0, 4, 118, 12, 0, 116, 66, 87, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> それ だけ なら まだ いい です けど 一番 <unk> の は コンクール の 日 は 学校 の <unk> の 前 で <unk> なけれ ば なり ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ほかのユーザによって自分のアカウントで添削した箇所に関してのコメントを見ることができないことを指摘されました。\n",
      "[2, 209, 4, 1532, 5, 1455, 7, 59, 4, 772, 12, 1890, 13, 8, 1960, 5, 2081, 7, 4, 158, 10, 197, 22, 9, 57, 18, 22, 10, 1804, 51, 41, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> ほか の ユーザ に よっ て 自分 の アカウント で 添削 し た 箇所 に 関し て の コメント を 見る こと が でき ない こと を 指摘 さ れ まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 例えば、まだ、ヤンキーが人気な反抗の文化で、アメリカの‘yankee’という単語から作られる。\n",
      "[2, 1613, 117, 180, 9, 148, 19, 396, 4, 60, 12, 89, 4, 1293, 1336, 1289, 11, 31, 311, 23, 1610, 214, 3]\n",
      "\n",
      "src (indice -> word): <sos> 例えば まだ ヤンキー が 人気 な 反抗 の 文化 で アメリカ の ` yankee ' と いう 単語 から 作ら れる <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 今日わたしはえいがかんで「インビクタス」という映画を見に行きました。\n",
      "[2, 97, 503, 6, 1356, 20, 1460, 1468, 11, 31, 75, 10, 104, 5, 455, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 今日 わたし は えいが か んで インビクタス と いう 映画 を 見 に 行き まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 施耐庵に対し、そんな頑固な性格によりやられるのではないかと心配してやまない。\n",
      "[2, 425, 452, 417, 5, 223, 174, 2097, 19, 0, 5, 157, 178, 214, 4, 12, 6, 18, 20, 11, 1022, 13, 7, 0, 18, 3]\n",
      "\n",
      "src (indice -> word): <sos> 施 耐 庵 に 対し そんな 頑固 な <unk> に より やら れる の で は ない か と 心配 し て <unk> ない <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ハノイで行われたヒェン・ツクさんのライブショーに参加したばかりだが、「サイゴン・ラブ・ストーリー」の俳優のフア・ビー・バンはもうすぐ開催される上海国際フィルムフェスティバルの準備で忙しそうだ。\n",
      "[2, 1501, 12, 456, 41, 8, 1511, 34, 4, 1534, 1486, 5, 395, 13, 8, 367, 27, 9, 509, 58, 1536, 58, 147, 4, 890, 4, 1514, 6, 142, 205, 2078, 51, 214, 860, 403, 1515, 1516, 4, 1894, 12, 1775, 81, 27, 3]\n",
      "\n",
      "src (indice -> word): <sos> ハノイ で 行わ れ た ヒェン・ツク さん の ライブ ショー に 参加 し た ばかり だ が サイゴン ・ ラブ ・ ストーリー の 俳優 の フア・ビー・バン は もう すぐ 開催 さ れる 上海 国際 フィルム フェスティバル の 準備 で 忙し そう だ <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: そして、私の一番の夢はタイへ行って、タイ語で現地の人と交流することです。\n",
      "[2, 73, 25, 4, 67, 1930, 4, 314, 6, 808, 141, 122, 7, 808, 33, 12, 1916, 4, 26, 11, 305, 29, 22, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> そして 私 の 一 番 の 夢 は タイ へ 行っ て タイ 語 で 現地 の 人 と 交流 する こと です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 口を開けた途端、マイクに故障があるようで、変な音が出てきました。\n",
      "[2, 0, 10, 0, 8, 0, 0, 5, 0, 9, 42, 40, 12, 967, 19, 639, 9, 391, 7, 86, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> を <unk> た <unk> <unk> に <unk> が ある よう で 変 な 音 が 出 て き まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ケーキもあれば、ビールもありました。皆、酒に酔うとゲームをしていました。有名なゲーム、[本当な話、大冒険]、他人の内緒を知りたいことは皆の興味ですね。\n",
      "[2, 0, 14, 1344, 66, 380, 14, 36, 16, 8, 90, 0, 5, 0, 11, 216, 10, 13, 7, 24, 16, 8, 431, 19, 216, 0, 152, 19, 171, 220, 909, 0, 872, 4, 0, 10, 0, 63, 22, 6, 90, 4, 1184, 15, 88, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> も あれ ば ビール も あり まし た 皆 <unk> に <unk> と ゲーム を し て い まし た 有名 な ゲーム <unk> 本当 な 話 大 冒険 <unk> 他人 の <unk> を <unk> たい こと は 皆 の 興味 です ね <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: でも、ある時、成功するために、私も一度決めたら他人のアドバイスを聞かない頑固な人だけでなく、気もよくいらいらする人になります。\n",
      "[2, 12, 14, 42, 49, 1796, 29, 82, 5, 25, 14, 67, 224, 597, 38, 872, 4, 1462, 10, 616, 18, 2097, 19, 26, 61, 12, 52, 98, 14, 70, 1351, 29, 26, 5, 87, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> で も ある 時 成功 する ため に 私 も 一 度 決め たら 他人 の アドバイス を 聞か ない 頑固 な 人 だけ で なく 気 も よく いらいら する 人 に なり ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: でも、信用が　　ありましたから、ついにEdison は沢山必要な物をはつめいしました。\n",
      "[2, 12, 14, 1615, 9, 36, 16, 8, 23, 722, 235, 6, 437, 266, 19, 337, 10, 1420, 13, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> で も 信用 が あり まし た から ついに edison は 沢山 必要 な 物 を はつめい し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: さらに、アメリカの最低賃金は先進国の中で最も低い賃金の一つだ。\n",
      "[2, 485, 89, 4, 590, 464, 6, 1622, 124, 4, 45, 12, 1836, 1608, 464, 4, 67, 208, 27, 3]\n",
      "\n",
      "src (indice -> word): <sos> さらに アメリカ の 最低 賃金 は 先進 国 の 中 で 最も 低い 賃金 の 一 つ だ <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: それは花嫁のご両親にされた障害を克服しながら、愛を証明するという意味です。\n",
      "[2, 44, 6, 454, 4, 698, 1565, 5, 51, 41, 8, 1269, 10, 1624, 13, 291, 1789, 10, 2019, 29, 11, 31, 226, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> それ は 花嫁 の ご 両親 に さ れ た 障害 を 克服 し ながら 愛 を 証明 する と いう 意味 です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 翻訳は難しいというのがわかっていますけど、私自身も翻訳（日ー＞中）の授業があるので、宿題とかもやっています。\n",
      "[2, 275, 6, 351, 11, 31, 4, 9, 502, 7, 24, 17, 39, 25, 618, 14, 275, 47, 1545, 45, 4, 151, 9, 42, 4, 12, 565, 11, 20, 14, 298, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 翻訳 は 難しい と いう の が わかっ て い ます けど 私 自身 も 翻訳 日 ー 中 の 授業 が ある の で 宿題 と か も やっ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 正直に言うと私は音痴の上に、たくさんの人の前で大声で話すことさえできませんでした。\n",
      "[2, 1864, 5, 1209, 11, 25, 6, 2096, 4, 159, 5, 92, 4, 26, 4, 118, 12, 973, 12, 2020, 22, 701, 57, 35, 28, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 正直 に 言う と 私 は 音痴 の 上 に たくさん の 人 の 前 で 大声 で 話す こと さえ でき ませ ん でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 実は、私は「両国の若者ともある程度自分の国の伝統文化をゆるがせにしている。」と思っています。\n",
      "[2, 318, 6, 25, 6, 181, 4, 114, 290, 42, 1157, 59, 4, 124, 4, 149, 60, 10, 1452, 5, 13, 7, 30, 11, 132, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 実 は 私 は 両国 の 若者 とも ある 程度 自分 の 国 の 伝統 文化 を ゆるがせ に し て いる と 思っ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 実はね、私も一年ぐらい日本語の勉強をしました。でも、あんまりできません。\n",
      "[2, 318, 6, 88, 25, 14, 67, 110, 484, 21, 33, 4, 119, 10, 13, 16, 8, 12, 14, 0, 57, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 実 は ね 私 も 一 年 ぐらい 日本 語 の 勉強 を し まし た で も <unk> でき ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: それは彼女を迎えに行ったせいでテレビを見れなかったので怒りました。\n",
      "[2, 44, 6, 120, 10, 0, 5, 122, 8, 488, 12, 254, 10, 2010, 62, 8, 4, 12, 1024, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> それ は 彼女 を <unk> に 行っ た せい で テレビ を 見れ なかっ た の で 怒り まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 帰る途中、私達は天津の有名なお菓子「耳の目」という揚げ餅を買いました。\n",
      "[2, 1754, 2055, 25, 630, 6, 1716, 4, 431, 19, 56, 232, 1177, 4, 339, 11, 31, 1812, 10, 346, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 帰る 途中 私 達 は 天津 の 有名 な お 菓子 耳 の 目 と いう 揚げ餅 を 買い まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 昨日私は近い川に行った時、涼しい水に足を置いて石に座りました。\n",
      "[2, 1070, 25, 6, 2048, 568, 5, 122, 8, 49, 1109, 1876, 5, 2035, 10, 0, 7, 0, 5, 0, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 昨日 私 は 近い 川 に 行っ た 時 涼しい 水 に 足 を <unk> て <unk> に <unk> まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: というわけで、中訳と原作をちょっと比較してみると、中訳はたしかに村上春樹さんの書いたように書いてあるんですけど、たぶん原作を忠実に再現するつもりだったかなと思いました。\n",
      "[2, 11, 31, 252, 12, 45, 198, 11, 312, 10, 69, 1871, 13, 7, 1442, 11, 45, 198, 6, 1395, 5, 331, 328, 34, 4, 125, 8, 40, 5, 125, 7, 42, 28, 15, 39, 289, 312, 10, 1776, 5, 1631, 29, 726, 83, 8, 20, 19, 11, 54, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> と いう わけ で 中 訳 と 原作 を ちょっと 比較 し て みる と 中 訳 は たしか に 村上 春樹 さん の 書い た よう に 書い て ある ん です けど たぶん 原作 を 忠実 に 再現 する つもり だっ た か な と 思い まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: PMに助けを求めて、サーバー管理とアカウント管理について、日本側の担当者と連携して、一緒に開発環境を守りました。\n",
      "[2, 1325, 5, 537, 10, 596, 7, 1482, 613, 11, 772, 613, 5, 123, 7, 21, 218, 4, 1042, 126, 11, 2063, 13, 7, 102, 5, 2079, 1919, 10, 1727, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> pm に 助け を 求め て サーバー 管理 と アカウント 管理 に つい て 日本 側 の 担当 者 と 連携 し て 一緒 に 開発 環境 を 守り まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 何と不思議なことに、虎たちは兵士たちと民衆たちに加害もせず、一目散に虎のキングを伴って開封府の大堂についた。\n",
      "[2, 79, 11, 257, 19, 22, 5, 196, 76, 6, 904, 76, 11, 1873, 76, 5, 1649, 14, 128, 127, 1552, 5, 196, 4, 793, 10, 1603, 7, 636, 1015, 4, 972, 5, 123, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 何 と 不思議 な こと に 虎 たち は 兵士 たち と 民衆 たち に 加害 も せ ず 一目散 に 虎 の キング を 伴っ て 開封 府 の 大堂 に つい た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 以下のようなフレーズがあります。\"自分を信じるわ\"や\"宝くじ当たったの？\n",
      "[2, 1594, 4, 40, 19, 1517, 9, 36, 17, 59, 10, 1614, 1459, 1736, 1767, 8, 4, 3]\n",
      "\n",
      "src (indice -> word): <sos> 以下 の よう な フレーズ が あり ます 自分 を 信じる わや 宝くじ 当たっ た の <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ある日、夏休みの最後の日、たろうとゆうたは森に冒険しに行くことにした。\n",
      "[2, 42, 47, 555, 4, 190, 4, 47, 207, 11, 299, 6, 269, 5, 909, 13, 5, 279, 22, 5, 13, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> ある 日 夏休み の 最後 の 日 たろう と ゆうた は 森 に 冒険 し に 行く こと に し た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: いつか、私たちの東洋的な若者文化はきっと世界中でブームになります。\n",
      "[2, 91, 20, 25, 76, 4, 1846, 113, 19, 114, 60, 6, 688, 303, 45, 12, 518, 5, 87, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> いつ か 私 たち の 東洋 的 な 若者 文化 は きっと 世界 中 で ブーム に なり ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: お客さんが皆入れるように大きいレストランとダンスホールを予約しておきました。\n",
      "[2, 56, 992, 34, 9, 90, 0, 40, 5, 407, 847, 11, 0, 0, 10, 1578, 13, 7, 674, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> お 客 さん が 皆 <unk> よう に 大きい レストラン と <unk> <unk> を 予約 し て おき まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 今日は両国のリーダーがやっとコミュニケーションをとったそうです。\n",
      "[2, 97, 6, 181, 4, 1537, 9, 763, 1478, 10, 100, 8, 81, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 今日 は 両国 の リーダー が やっと コミュニケーション を とっ た そう です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 昨日、チェコの「スロウプスコ・ショシューフスケー」洞に遠足しました。\n",
      "[2, 1070, 0, 4, 0, 1105, 5, 0, 13, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 昨日 <unk> の <unk> 洞 に <unk> し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 学生の時といわず、社会人の時といわず、私は穏やかで気品のある話し方をする人であることが変わりません。\n",
      "[2, 185, 4, 49, 11, 0, 127, 135, 26, 4, 49, 11, 0, 127, 25, 6, 0, 12, 0, 4, 42, 461, 112, 10, 29, 26, 12, 42, 22, 9, 0, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 学生 の 時 と <unk> ず 社会 人 の 時 と <unk> ず 私 は <unk> で <unk> の ある 話し 方 を する 人 で ある こと が <unk> ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 昨日、チェコの「スロウプスコ・ショシューフスケー」洞に遠足しました。\n",
      "[2, 1070, 0, 4, 0, 1105, 5, 0, 13, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 昨日 <unk> の <unk> 洞 に <unk> し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: また、日本のロックバンド及びミュージシャンから始まって、過激で派手な化粧や髪型、衣装などを特徴としている「ヴィジュアル系」も私はどうしても受け入れられません。\n",
      "[2, 296, 21, 4, 1542, 1505, 933, 1526, 23, 1721, 7, 1250, 12, 1886, 19, 541, 65, 2108, 2004, 130, 10, 604, 11, 13, 7, 30, 849, 1165, 14, 25, 6, 64, 13, 7, 14, 1669, 96, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> また 日本 の ロック バンド 及び ミュージシャン から 始まっ て 過激 で 派手 な 化粧 や 髪型 衣装 など を 特徴 と し て いる ヴィジュアル 系 も 私 は どう し て も 受け入れ られ ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 本地の書店でいろいろな辞書を見ましたがいい辞典はなさそうです。\n",
      "[2, 0, 4, 0, 12, 201, 19, 172, 10, 104, 16, 8, 9, 50, 0, 6, 0, 81, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> の <unk> で いろいろ な 辞書 を 見 まし た が いい <unk> は <unk> そう です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: やはり言葉を身につけると、世界が広がったり、考えも変わったりしますよね。\n",
      "[2, 499, 80, 10, 627, 5, 1403, 11, 303, 9, 1012, 93, 121, 14, 554, 93, 13, 17, 109, 88, 3]\n",
      "\n",
      "src (indice -> word): <sos> やはり 言葉 を 身 に つける と 世界 が 広がっ たり 考え も 変わっ たり し ます よ ね <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 実はわが国では、出かけるときカバンを持って行く習慣がないと思う。\n",
      "[2, 318, 6, 0, 124, 12, 6, 0, 176, 504, 10, 111, 7, 279, 0, 9, 18, 11, 166, 3]\n",
      "\n",
      "src (indice -> word): <sos> 実 は <unk> 国 で は <unk> とき カバン を 持っ て 行く <unk> が ない と 思う <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: たとえびんぼうな家族でも希望がありましたから、ついに本田宗一郎は有名なオートバイの会社のかんりしゃになりました。\n",
      "[2, 1397, 1429, 19, 165, 12, 14, 1008, 9, 36, 16, 8, 23, 722, 1843, 1733, 6, 431, 19, 785, 4, 162, 4, 1366, 1457, 705, 5, 87, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> たとえ びんぼう な 家族 で も 希望 が あり まし た から ついに 本田 宗一郎 は 有名 な オートバイ の 会社 の かん り しゃ に なり まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: お医者さんに会ったけど彼は「あなたは元気ですが、ティーンエイジャーだからいつも眠いのですね。」と言いました。\n",
      "[2, 56, 1655, 34, 5, 1600, 8, 39, 188, 6, 663, 6, 893, 15, 9, 1494, 27, 23, 91, 14, 1947, 4, 15, 88, 11, 137, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> お 医者 さん に 会っ た けど 彼 は あなた は 元気 です が ティーンエイジャー だ から いつ も 眠い の です ね と 言い まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: なぜそう名づけたかというと、彼の飼い主は回文が好きだからです。\n",
      "[2, 1414, 81, 0, 8, 20, 11, 31, 11, 188, 4, 0, 6, 0, 9, 68, 27, 23, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> なぜ そう <unk> た か と いう と 彼 の <unk> は <unk> が 好き だ から です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: （「I work as an assistant teacher」は日本語でなんと言うのでしょうか。）\n",
      "[2, 1311, 1334, 1298, 1297, 1299, 1327, 6, 21, 33, 12, 292, 11, 1209, 4, 140, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> i work as an assistant teacher は 日本 語 で なん と 言う の でしょう か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私たちは　五日間　過ごした。　最初の三日間は　雨が　非常に　降って　大変でした。\n",
      "[2, 25, 76, 6, 1583, 1067, 348, 8, 228, 4, 858, 1067, 6, 0, 9, 2094, 5, 0, 7, 408, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 たち は 五 日間 過ごし た 最初 の 三 日間 は <unk> が 非常 に <unk> て 大変 でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ③中国が仕事で中国へ出向いていたフジタの社員を逮捕した上、さらに船長を解放しなければ軍事的行為も辞さないと通告\n",
      "[2, 53, 9, 1590, 12, 53, 141, 0, 7, 24, 8, 0, 4, 0, 10, 0, 13, 8, 159, 485, 619, 10, 0, 13, 116, 66, 2041, 113, 2002, 14, 0, 18, 11, 2059, 3]\n",
      "\n",
      "src (indice -> word): <sos> 中国 が 仕事 で 中国 へ <unk> て い た <unk> の <unk> を <unk> し た 上 さらに 船長 を <unk> し なけれ ば 軍事 的 行為 も <unk> ない と 通告 <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: この文章はキャノン杯スピーチ特訓班の発表のテーマとして、書いています。\n",
      "[2, 43, 587, 6, 790, 1080, 1492, 1910, 606, 4, 608, 4, 815, 11, 13, 7, 125, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> この 文章 は キャノン 杯 スピーチ 特訓 班 の 発表 の テーマ と し て 書い て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私の仕事は技術サポータなので、今週も休暇をとることができません。\n",
      "[2, 25, 4, 1590, 6, 1801, 1481, 19, 4, 12, 527, 14, 1598, 10, 1409, 22, 9, 57, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 の 仕事 は 技術 サポータ な の で 今週 も 休暇 を とる こと が でき ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 毎日授業が終わった後、一人でＣＤを聞いて、歌詞を暗記しました。\n",
      "[2, 230, 151, 9, 1169, 8, 103, 521, 12, 1303, 10, 136, 7, 592, 10, 1834, 13, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 毎日 授業 が 終わっ た 後 一人 で cd を 聞い て 歌詞 を 暗記 し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 僕は「たちばな」で食べていた時、ホステスとシェフの会話が分かってうれしかった。\n",
      "[2, 131, 6, 1396, 12, 199, 7, 24, 8, 49, 1521, 11, 1483, 4, 1601, 9, 1636, 7, 1354, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 僕 は たちばな で 食べ て い た 時 ホステス と シェフ の 会話 が 分かっ て うれしかっ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 今両国ともはやっている「萌え文化」とか「ツッコミ文化」には私もすごく興味があります。\n",
      "[2, 78, 181, 11, 14, 1423, 7, 30, 1996, 60, 11, 20, 812, 60, 5, 6, 25, 14, 243, 1184, 9, 36, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 今 両国 と も はやっ て いる 萌え 文化 と か ツッコミ 文化 に は 私 も すごく 興味 が あり ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: だいがくのクラブの Japanese Student Association (JSA) がホストです。\n",
      "[2, 1398, 4, 506, 4, 236, 1326, 1300, 105, 1313, 106, 9, 1522, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> だいがく の クラブ の japanese student association ( jsa ) が ホスト です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 初期の映画から今のドラマ、アニメ、テレビゲーム及び一部の電子製品に至るまで、日本文化の影響が至る所に見られています。\n",
      "[2, 1643, 4, 75, 23, 78, 4, 255, 144, 254, 216, 933, 522, 4, 2090, 2008, 5, 1986, 94, 21, 60, 4, 322, 9, 1987, 5, 104, 96, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 初期 の 映画 から 今 の ドラマ アニメ テレビ ゲーム 及び 一部 の 電子 製品 に 至る まで 日本 文化 の 影響 が 至る所 に 見 られ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 国内の皆さんもそれを知っていると思います。実は国内で今回の中国政府の抗議しかしないやり方が弱いと批判されています。\n",
      "[2, 262, 4, 90, 34, 14, 44, 10, 1145, 7, 30, 11, 54, 17, 318, 6, 262, 12, 258, 4, 53, 167, 4, 422, 288, 13, 18, 500, 112, 9, 1764, 11, 582, 51, 41, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 国内 の 皆 さん も それ を 知っ て いる と 思い ます 実 は 国内 で 今回 の 中国 政府 の 抗議 しか し ない やり 方 が 弱い と 批判 さ れ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 口を開けた途端、マイクに故障があるようで、変な音が出てきました。\n",
      "[2, 0, 10, 0, 8, 0, 0, 5, 0, 9, 42, 40, 12, 967, 19, 639, 9, 391, 7, 86, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> を <unk> た <unk> <unk> に <unk> が ある よう で 変 な 音 が 出 て き まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 皆さんに「京劇」という言葉を聞いてどういったことを思い浮かべるかという質問をしてみたら、皆さんはどう答えるつもりですか？\n",
      "[2, 90, 34, 5, 1586, 11, 31, 80, 10, 136, 7, 64, 358, 8, 22, 10, 1026, 20, 11, 31, 465, 10, 13, 7, 95, 38, 90, 34, 6, 64, 448, 726, 15, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> 皆 さん に 京劇 と いう 言葉 を 聞い て どう いっ た こと を 思い浮かべる か と いう 質問 を し て み たら 皆 さん は どう 答える つもり です か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 次の日も結婚式は続いていましたが、私は大きい音楽で頭痛がしたので、帰りました。\n",
      "[2, 334, 4, 47, 14, 195, 225, 6, 615, 7, 24, 16, 8, 9, 25, 6, 407, 640, 12, 2098, 9, 13, 8, 4, 12, 1753, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 次 の 日 も 結婚 式 は 続い て い まし た が 私 は 大きい 音楽 で 頭痛 が し た の で 帰り まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 乗り物とアトラクションのほかに、公園の中には、多くのファストフードと土産店もあります。\n",
      "[2, 1574, 11, 301, 4, 209, 5, 219, 4, 45, 5, 6, 1710, 4, 1513, 1518, 11, 1695, 1014, 14, 36, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 乗り物 と アトラクション の ほか に 公園 の 中 に は 多く の ファスト フード と 土産 店 も あり ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: アメリカの給料レポートによると、アメリカのCEOの給料は、労働者の４７５倍の給料だということだ 。\n",
      "[2, 89, 4, 449, 520, 5, 250, 11, 89, 4, 284, 4, 449, 6, 538, 126, 4, 32, 1616, 4, 449, 27, 11, 31, 22, 27, 3]\n",
      "\n",
      "src (indice -> word): <sos> アメリカ の 給料 レポート に よる と アメリカ の ceo の 給料 は 労働 者 の 0 倍 の 給料 だ と いう こと だ <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: そして10時くらいに、喫茶店でレーシャルとジョノサンとベルに会いました。\n",
      "[2, 73, 32, 49, 287, 5, 1687, 1014, 12, 1541, 1485, 11, 1490, 11, 1520, 5, 1599, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> そして 0 時 くらい に 喫茶 店 で レー シャル と ジョノサン と ベル に 会い まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: カバンを持っている人はたぶんいろいろなものを入れておくと便利だと考える節があるが、もう一つは若い者は衣服を飾ることだと思う。\n",
      "[2, 504, 10, 111, 7, 30, 26, 6, 289, 201, 19, 77, 10, 0, 7, 0, 11, 0, 27, 11, 0, 0, 9, 42, 9, 142, 67, 208, 6, 1190, 126, 6, 0, 10, 0, 22, 27, 11, 166, 3]\n",
      "\n",
      "src (indice -> word): <sos> カバン を 持っ て いる 人 は たぶん いろいろ な もの を <unk> て <unk> と <unk> だ と <unk> <unk> が ある が もう 一 つ は 若い 者 は <unk> を <unk> こと だ と 思う <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私は日本語がとても好きです。そして私はここでしばしば書くつもりです。\n",
      "[2, 25, 6, 21, 33, 9, 74, 68, 15, 73, 25, 6, 240, 12, 0, 189, 726, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 は 日本 語 が とても 好き です そして 私 は ここ で <unk> 書く つもり です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: うれしい限りですが、知らない言葉はまだいっぱいありますので、辞書を引くのは相変わらず必要なぐらいでした。\n",
      "[2, 481, 2082, 15, 9, 1146, 18, 80, 6, 117, 480, 36, 17, 4, 12, 172, 10, 1763, 4, 6, 1944, 1707, 127, 266, 19, 484, 46, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> うれしい 限り です が 知ら ない 言葉 は まだ いっぱい あり ます の で 辞書 を 引く の は 相 変わら ず 必要 な ぐらい でし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 「勉強すればするほど、難しいと思います」とよく聞いたが、今ならこのセンテンスはちょっとだけわかるようになった。\n",
      "[2, 119, 0, 66, 29, 177, 351, 11, 54, 17, 11, 70, 136, 8, 9, 78, 108, 43, 807, 6, 69, 61, 371, 40, 5, 55, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 勉強 <unk> ば する ほど 難しい と 思い ます と よく 聞い た が 今 なら この センテンス は ちょっと だけ わかる よう に なっ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: みんながこの世の中に使命を負わなければならないと思っています。\n",
      "[2, 213, 9, 43, 1563, 5, 1611, 10, 2028, 116, 66, 108, 18, 11, 132, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> みんな が この 世の中 に 使命 を 負わ なけれ ば なら ない と 思っ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: アメリカのCEOと生産労働者の総年収の格差は大きすぎる。そのシステムはおかしくないだろうか？\n",
      "[2, 89, 4, 284, 11, 1925, 538, 126, 4, 1971, 1756, 4, 1085, 6, 1712, 1386, 48, 1484, 6, 1358, 18, 129, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> アメリカ の ceo と 生産 労働 者 の 総 年収 の 格差 は 大き すぎる その システム は おかしく ない だろう か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 馬の　名前は　Ｂｉｓｓｅｎ、　Ｐｕｎｔｏ、　Ｍａｙ　と　Ｃａｐｐｉ　です。\n",
      "[2, 2102, 4, 313, 6, 1301, 11, 1302, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 馬 の 名前 は bissenpuntomay と cappi です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 突然床に不明な嘔吐されたものを見つけました。プリンの食欲がだんだん少なくなって、病気になって、入院しました。\n",
      "[2, 612, 1760, 5, 524, 19, 1689, 51, 41, 8, 77, 10, 624, 16, 8, 381, 4, 2101, 9, 492, 415, 55, 7, 338, 5, 55, 7, 900, 13, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 突然 床 に 不明 な 嘔吐 さ れ た もの を 見つけ まし た プリン の 食欲 が だんだん 少なく なっ て 病気 に なっ て 入院 し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: たろうはいつものように、古い本を探していると、棚の奥から一冊のボロボロした本を見つけた。\n",
      "[2, 207, 6, 91, 14, 4, 40, 5, 0, 134, 10, 326, 7, 30, 11, 0, 4, 559, 23, 67, 907, 4, 0, 13, 8, 134, 10, 624, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> たろう は いつ も の よう に <unk> 本 を 探し て いる と <unk> の 奥 から 一 冊 の <unk> し た 本 を 見つけ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私たちはこの学期の毎土曜日の午後に体育館のバドミントンの場所を予約しているので、今日からそこに行けます。\n",
      "[2, 25, 76, 6, 43, 1726, 4, 1869, 1694, 47, 4, 1657, 5, 881, 473, 4, 820, 4, 553, 10, 1578, 13, 7, 30, 4, 12, 97, 23, 361, 5, 622, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 たち は この 学期 の 毎 土曜 日 の 午後 に 体育 館 の バドミントン の 場所 を 予約 し て いる の で 今日 から そこ に 行け ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 地下鉄に乗る時、お年寄りとか、中年の人とか、若者とか、学生とか、誰でもカバンを持っていることだった。\n",
      "[2, 0, 0, 5, 869, 49, 56, 0, 11, 20, 0, 4, 26, 11, 20, 114, 11, 20, 185, 11, 20, 1225, 12, 14, 504, 10, 111, 7, 30, 22, 83, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> <unk> <unk> に 乗る 時 お <unk> と か <unk> の 人 と か 若者 と か 学生 と か 誰 で も カバン を 持っ て いる こと だっ た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 今日は最初のポストを送りました。　そして五人がすぐに答えました。\n",
      "[2, 97, 6, 228, 4, 1523, 10, 2053, 16, 8, 73, 1583, 26, 9, 205, 5, 341, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 今日 は 最初 の ポスト を 送り まし た そして 五 人 が すぐ に 答え まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 日本の「ロリータファッション」とか「ヴィジュアル系」とか外国人の私としてはちょっと理解しがたい文化は、中国にもあります。\n",
      "[2, 21, 4, 385, 179, 11, 20, 849, 1165, 11, 20, 150, 26, 4, 25, 11, 13, 7, 6, 69, 169, 13, 1367, 60, 6, 53, 5, 14, 36, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 日本 の ロリータ ファッション と か ヴィジュアル 系 と か 外国 人 の 私 と し て は ちょっと 理解 し がたい 文化 は 中国 に も あり ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 西洋人にとって、愛情はすべて！」......先生はそう言ったけど、実際の状況は本当にそうなのか。\n",
      "[2, 1199, 26, 5, 100, 7, 0, 6, 1388, 37, 37, 37, 37, 37, 37, 84, 6, 81, 138, 8, 39, 0, 4, 1912, 6, 152, 5, 81, 19, 4, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> 西洋 人 に とっ て <unk> は すべて . . . . . . 先生 は そう 言っ た けど <unk> の 状況 は 本当 に そう な の か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: その公園の中で、多くのアトラクションと乗り物を楽しむことができるので、オーストラリアの中で、その公園は一番すごいそうです。\n",
      "[2, 48, 219, 4, 45, 12, 1710, 4, 301, 11, 1574, 10, 0, 22, 9, 107, 4, 12, 0, 4, 45, 12, 48, 219, 6, 71, 708, 81, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> その 公園 の 中 で 多く の アトラクション と 乗り物 を <unk> こと が できる の で <unk> の 中 で その 公園 は 一番 すごい そう です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私たちはますます自国の伝統文化を失っているのではないか？」と友達が私にそういいました。\n",
      "[2, 25, 76, 6, 1437, 617, 4, 149, 60, 10, 558, 7, 30, 4, 12, 6, 18, 20, 11, 72, 9, 25, 5, 81, 50, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 たち は ますます 自国 の 伝統 文化 を 失っ て いる の で は ない か と 友達 が 私 に そう いい まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: でも、そのイベントの前から、チッソ株式会社は継続的に汚染していた。\n",
      "[2, 12, 14, 48, 1467, 4, 118, 23, 514, 591, 162, 6, 1969, 113, 5, 1877, 13, 7, 24, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> で も その イベント の 前 から チッソ 株式 会社 は 継続 的 に 汚染 し て い た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私の一番好きなテレビ番組は「Torchwood」と「Doctor Who」です。\n",
      "[2, 25, 4, 71, 68, 19, 254, 1931, 6, 1330, 11, 1304, 1332, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 の 一番 好き な テレビ 番組 は torchwood と doctor who です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: そして、二人は新しい冒険のために、また次の夏休みを楽しみにした。\n",
      "[2, 73, 161, 6, 589, 909, 4, 82, 5, 296, 334, 4, 555, 10, 1855, 5, 13, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> そして 二人 は 新しい 冒険 の ため に また 次 の 夏休み を 楽しみ に し た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ヤンキーのリーダーはせんぱいで、こうはいはせんぱいに話す時に、敬語を使わなければならない。\n",
      "[2, 180, 4, 1537, 6, 0, 12, 0, 6, 0, 5, 2020, 49, 5, 1821, 10, 0, 116, 66, 108, 18, 3]\n",
      "\n",
      "src (indice -> word): <sos> ヤンキー の リーダー は <unk> で <unk> は <unk> に 話す 時 に 敬語 を <unk> なけれ ば なら ない <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 漢字で　かくことが　はじめて　です。　だから　間違いなく、　誤記が　有ります。\n",
      "[2, 336, 12, 1363, 22, 9, 1419, 15, 27, 23, 637, 52, 2022, 9, 1838, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 漢字 で かく こと が はじめて です だ から 間違い なく 誤記 が 有り ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私は高校三年生だ、もうすぐ大学入学試験があるから、毎日勉強しなければならない、大変です！\n",
      "[2, 25, 6, 353, 858, 110, 1920, 27, 142, 205, 263, 0, 460, 9, 42, 23, 230, 119, 13, 116, 66, 108, 18, 408, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 は 高校 三 年 生 だ もう すぐ 大学 <unk> 試験 が ある から 毎日 勉強 し なけれ ば なら ない 大変 です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 福山と柴崎はもちろん好きだし、北村一輝と松雪泰子も昔から気に入る俳優で、一緒に出演していて嬉しかったです。\n",
      "[2, 1954, 11, 1852, 6, 1445, 68, 27, 13, 1652, 1555, 11, 1848, 1881, 14, 227, 23, 98, 5, 308, 890, 12, 102, 5, 1634, 13, 7, 24, 7, 561, 8, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> 福山 と 柴崎 は もちろん 好き だ し 北村 一輝 と 松雪 泰子 も 昔 から 気 に 入る 俳優 で 一緒 に 出演 し て い て 嬉しかっ た です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: そして、彼は戦争の後、両国はすぐ仲直りすることも予言しました。\n",
      "[2, 73, 188, 6, 1797, 4, 103, 181, 6, 205, 0, 29, 22, 14, 0, 13, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> そして 彼 は 戦争 の 後 両国 は すぐ <unk> する こと も <unk> し まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: おばあちゃんのせりふの半分以上わかんないというときは少なくなかったよ。\n",
      "[2, 56, 496, 244, 4, 1391, 4, 544, 387, 1458, 18, 11, 31, 176, 6, 415, 62, 8, 109, 3]\n",
      "\n",
      "src (indice -> word): <sos> お ばあ ちゃん の せりふ の 半分 以上 わかん ない と いう とき は 少なく なかっ た よ <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 帰ったあと兄は「インターネットで買うのはどうだ。」と言いました。僕は一度もインターネットで買い物をした経験がないから、すべてを兄に手伝ってもらいました。\n",
      "[2, 320, 8, 477, 390, 6, 215, 12, 1232, 4, 6, 64, 27, 11, 137, 16, 8, 131, 6, 67, 224, 14, 215, 12, 2029, 10, 13, 8, 1967, 9, 18, 23, 1388, 10, 390, 5, 1799, 7, 1447, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 帰っ た あと 兄 は インターネット で 買う の は どう だ と 言い まし た 僕 は 一 度 も インターネット で 買い物 を し た 経験 が ない から すべて を 兄 に 手伝っ て もらい まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: それにつれて、高エネルギーの粒子が放出されて、地球の磁場がめっちゃ乱れたみたいで、磁気嵐が起こったらしい。\n",
      "[2, 44, 5, 365, 7, 0, 0, 4, 0, 9, 0, 51, 41, 7, 0, 4, 0, 9, 1443, 0, 8, 212, 12, 0, 0, 9, 0, 8, 370, 3]\n",
      "\n",
      "src (indice -> word): <sos> それ に つれ て <unk> <unk> の <unk> が <unk> さ れ て <unk> の <unk> が めっちゃ <unk> た みたい で <unk> <unk> が <unk> た らしい <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: アメリカにとって有利にことが運んだとしても、それは結果論であり、アメリカが何か目的を持って動いたわけでもなく、アメリカにとっては「漁夫の利」であり、中国にとっては「墓穴を掘った」ということなのではないでしょうか？\n",
      "[2, 89, 5, 100, 7, 1839, 5, 22, 9, 2068, 27, 11, 13, 7, 14, 44, 6, 1968, 2025, 12, 36, 89, 9, 79, 20, 444, 10, 111, 7, 1650, 8, 252, 12, 14, 52, 89, 5, 100, 7, 6, 1895, 4, 1646, 12, 36, 53, 5, 100, 7, 6, 1702, 10, 1809, 8, 11, 31, 22, 19, 4, 12, 6, 18, 140, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> アメリカ に とっ て 有利 に こと が 運ん だ と し て も それ は 結果 論 で あり アメリカ が 何 か 目的 を 持っ て 動い た わけ で も なく アメリカ に とっ て は 漁夫 の 利 で あり 中国 に とっ て は 墓穴 を 掘っ た と いう こと な の で は ない でしょう か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 「racecar」は、右から左に呼んでも同じ「racecar」なんです。\n",
      "[2, 652, 6, 1671, 23, 1751, 5, 1681, 12, 14, 398, 652, 19, 28, 15, 3]\n",
      "\n",
      "src (indice -> word): <sos> racecar は 右 から 左 に 呼ん で も 同じ racecar な ん です <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: でも、障害も難しいこともありますから、度々夢とか希望を果たすことができません。\n",
      "[2, 12, 14, 1269, 14, 351, 22, 14, 36, 17, 23, 224, 314, 11, 20, 1008, 10, 1849, 22, 9, 57, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> で も 障害 も 難しい こと も あり ます から 度 夢 と か 希望 を 果たす こと が でき ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ロシアで結婚届けを出した後、名所を見物することになっています。\n",
      "[2, 384, 12, 195, 1747, 10, 260, 8, 103, 948, 10, 2011, 29, 22, 5, 55, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> ロシア で 結婚 届け を 出し た 後 名所 を 見物 する こと に なっ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: ここで気をつけなければならないところは日本の法律はアメリカと同じです。今度の事件は日本国内法の手順で完全に裁判されました。\n",
      "[2, 240, 12, 98, 10, 0, 116, 66, 108, 18, 156, 6, 21, 4, 0, 6, 89, 11, 398, 15, 0, 4, 1580, 6, 21, 262, 1103, 4, 0, 12, 1731, 5, 344, 51, 41, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> ここ で 気 を <unk> なけれ ば なら ない ところ は 日本 の <unk> は アメリカ と 同じ です <unk> の 事件 は 日本 国内 法 の <unk> で 完全 に 裁判 さ れ まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 考えてみましょう、いまは中国の大事な発展の転換期、わざとこんな問題を起こす必要がありますか。\n",
      "[2, 121, 7, 95, 752, 359, 6, 53, 4, 0, 19, 1938, 4, 0, 0, 0, 139, 261, 10, 2033, 266, 9, 36, 17, 20, 3]\n",
      "\n",
      "src (indice -> word): <sos> 考え て み ましょう いま は 中国 の <unk> な 発展 の <unk> <unk> <unk> こんな 問題 を 起こす 必要 が あり ます か <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 映画の中で､小米さんはあまり自信がない女の子なので､彼女には恋する勇気がありません。\n",
      "[2, 75, 4, 45, 12, 319, 34, 6, 285, 453, 9, 18, 560, 19, 4, 12, 120, 5, 6, 574, 29, 539, 9, 36, 35, 28, 3]\n",
      "\n",
      "src (indice -> word): <sos> 映画 の 中 で 小米 さん は あまり 自信 が ない 女の子 な の で 彼女 に は 恋 する 勇気 が あり ませ ん <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: こんな話は年上から見ると、特に意味がない普通の話にすぎないかもしれませんけど、若者にとっては、ユーモラスなツッコミになりました。\n",
      "[2, 139, 171, 6, 569, 23, 197, 11, 440, 226, 9, 18, 428, 4, 171, 5, 155, 18, 20, 14, 99, 35, 28, 39, 114, 5, 100, 7, 6, 1533, 19, 812, 5, 87, 16, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> こんな 話 は 年上 から 見る と 特に 意味 が ない 普通 の 話 に すぎ ない か も しれ ませ ん けど 若者 に とっ て は ユーモラス な ツッコミ に なり まし た <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 完成したら、弟にコスチュームの写真を撮ってくれるように頼みます。\n",
      "[2, 1732, 13, 38, 321, 5, 1476, 4, 259, 10, 584, 7, 483, 40, 5, 1279, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 完成 し たら 弟 に コスチューム の 写真 を 撮っ て くれる よう に 頼み ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 子供の時から今まで、私はよく責任を引き受けて、真面目な人になるように育てられています。\n",
      "[2, 316, 4, 49, 23, 78, 94, 25, 6, 70, 1231, 10, 0, 7, 0, 19, 26, 5, 101, 40, 5, 0, 96, 7, 24, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 子供 の 時 から 今 まで 私 は よく 責任 を <unk> て <unk> な 人 に なる よう に <unk> られ て い ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 今大方クラシックギターを練習していて、時々エレクトリックギターを弾く。\n",
      "[2, 78, 1715, 1475, 794, 10, 450, 13, 7, 24, 7, 49, 1469, 794, 10, 1766, 3]\n",
      "\n",
      "src (indice -> word): <sos> 今 大方 クラシック ギター を 練習 し て い て 時 エレクトリック ギター を 弾く <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 私の中に鮮明に刻まれたあなたは、思い出さなくとも、自然と頭の中に浮かんできます。\n",
      "[2, 25, 4, 45, 5, 2109, 5, 1647, 41, 8, 663, 6, 573, 52, 290, 1985, 11, 352, 4, 45, 5, 438, 12, 86, 17, 3]\n",
      "\n",
      "src (indice -> word): <sos> 私 の 中 に 鮮明 に 刻ま れ た あなた は 思い出さ なく とも 自然 と 頭 の 中 に 浮かん で き ます <eos> \n",
      "'''''''''''''''''''''''''''''''''''''\n",
      "source sentence: 彼女の声が上がったり下がったり、僕がなれている話し方とぜんぜん違うものだった。\n",
      "[2, 120, 4, 1705, 9, 1558, 93, 1561, 93, 131, 9, 734, 7, 30, 461, 112, 11, 490, 349, 77, 83, 8, 3]\n",
      "\n",
      "src (indice -> word): <sos> 彼女 の 声 が 上がっ たり 下がっ たり 僕 が なれ て いる 話し 方 と ぜんぜん 違う もの だっ た <eos> "
     ]
    }
   ],
   "source": [
    "# Prepare translation for test data\n",
    "translations = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    predicted_translation = translation(X_test[i], model, train_X_vocab, train_y_vocab, train_dataset)\n",
    "    translations.append(predicted_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bde9586c-48a6-4efb-8439-151407c5ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize target sentences\n",
    "tokenized_trg_sentence = []\n",
    "\n",
    "for data in y_test:\n",
    "    cleaned_data = data_cleaning(data)\n",
    "    normalized_data = normalization(cleaned_data)\n",
    "    tokens = tokenization(normalized_data)\n",
    "    tokenized_trg_sentence.append(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "P3fQHrWeVm6Z",
   "metadata": {
    "id": "P3fQHrWeVm6Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence : 今日は午前９時に大学へ行こうとしたが、家の近くの道路が、工事中のため通行止めでした。\n",
      "\n",
      "Model Prediction: ['は', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'の', 'て', 'た', 'た', 'た', 'た', 'た', 'た', 'た']\n",
      "\n",
      "Target Sentence : ['今日', 'は', '午前', '0', '時', 'に', '大学', 'へ', '行こう', 'と', 'し', 'まし', 'た', 'が', '家', 'の', '近く', 'の', '道路', 'が', '工事', '中', 'の', 'ため', '通行', '止め', 'でし', 'た']\n"
     ]
    }
   ],
   "source": [
    "# # Show an example of source sentence, model-generated translation, and target sentence. \n",
    "print(\"Source Sentence :\", X_test[0])\n",
    "print(\"\\nModel Prediction:\", translations[0])\n",
    "print(\"\\nTarget Sentence :\", tokenized_trg_sentence[0])\n",
    "\n",
    "# print(translations)\n",
    "# print(tokenized_trg_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac54a1d-e993-4575-81a3-6186621c8423",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "This project **does *NOT* give a functional translation currently**. The translation function keeps generating the same few words at index 4, 6, and 8. Definitely a part of the model is incorrect. I have tried to see the input I am putting in for model, but everything looks good. This will require more time and effort to look into it. \n",
    "\n",
    "Besides the existing issues, I am looking into invesigating these parts to have this model improve its result in the future. \n",
    "* **Increase Dataset (Parallel Corpus)**: the suggested dataset needed is from 1,000 - 10,000. My dataset only has around ~850 pairs. Increasing the dataset will improve the training for sure.\n",
    "* **Try Various Combination of Variables**: different combincation of batch, hidden dimension, and embedding dimension can be tested to see which one gives out better results.\n",
    "\n",
    "If you want to provide some advice, please reach out to me. I will appreciate it:) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
